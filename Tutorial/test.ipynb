{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfcad00d-86c6-478c-a0b0-145f604489e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9c685fe-0365-400d-abbe-a3822f6d6bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da0c0bb-824b-4ff5-a550-902a5a2d07c7",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2b5d20-5c43-4278-b415-aeac04f99f9b",
   "metadata": {},
   "source": [
    "### PDF Retriever and Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dae17b2-5f4e-4bf9-b83f-0d5143964a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/SPRI_AI_Brief_2023년12월호_F.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    }
   ],
   "source": [
    "from rag.pdf import PDFRetrievalChain\n",
    "\n",
    "# PDF 문서를 로드합니다.\n",
    "pdf_chain_obj = PDFRetrievalChain([\"data/SPRI_AI_Brief_2023년12월호_F.pdf\"]).create_chain()\n",
    "\n",
    "# retriever와 chain을 생성합니다.\n",
    "pdf_retriever = pdf_chain_obj.retriever\n",
    "pdf_chain = pdf_chain_obj.chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4ff8e2-d6cb-47b7-ad7e-b0f7fda2ad7d",
   "metadata": {},
   "source": [
    "### Query Router (Vector store or Web Search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b42829-a3f9-4795-a00b-af97f9f8e70b",
   "metadata": {},
   "source": [
    "#### Route Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7451749a-aeb5-4d72-a848-215479151a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "system_message = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
    "                    The vectorstore contains documents related to DEC 2023 AI Brief Report(SPRI) with Samsung Gause, Anthropic, etc.\n",
    "                    Use the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\n",
    "\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_message),\n",
    "        (\"human\", \"{query}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150d7236-b355-4801-a0db-b5cb1b77829b",
   "metadata": {},
   "source": [
    "#### Query Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e157da8-e5ad-4a84-8bad-b0f8ebd83b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RouteQuery(BaseModel):\n",
    "    \"\"\" Route a user query to the most relevant datasource \"\"\"\n",
    "\n",
    "    datasource: Literal[\"vector_store\", \"web_search\"] = Field(\n",
    "        ..., # required\n",
    "        description= \"Route the user query to either the vectorstore or the web search engine based on its content.\"\n",
    "    )\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "structured_llm_router = llm.with_structured_output(RouteQuery) # Convert the result to RouteQuery type.\n",
    "\n",
    "query_router = route_prompt | structured_llm_router"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a285460b-77a3-4b54-b4af-14ecfbe7a362",
   "metadata": {},
   "source": [
    "#### Test Query Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5efd8065-5369-4edc-b701-261f93f1cda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasource='vector_store'\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    query_router.invoke(\n",
    "        {\"query\": \"AI Brief 에서 삼성전자가 만든 생성형 AI 의 이름은?\"}\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a38c4e6c-0c8c-45d4-9b09-cce25aa9228b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasource='web_search'\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    query_router.invoke(\n",
    "        {\"query\": \"판교에서 가장 맛있는 딤섬집 찾아줘\"}\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6c6d2d-2156-47d1-8a1b-f97ab0579e79",
   "metadata": {},
   "source": [
    "### Retrieval Grader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117cf622-8a1d-4d60-a1a3-6f63b8200b3d",
   "metadata": {},
   "source": [
    "#### System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad3a886f-09c4-4a4e-a6a1-a791921c3fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system = \"\"\" You are a grader evaluating the relevance of a retrieved document to a user query.\n",
    "            If the document contains keywords or semantic meanings related to the query, consider it relevant.\n",
    "            This is not a strict test — the goal is to filter out irrelevant results clearly.\n",
    "            Return a binary score: \"yes\" if the document is relevant, or \"no\" if it is not.\"\"\"\n",
    "\n",
    "retrieval_grader_prompt  = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {documents} \\n\\n User query: {query}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7344d998-2b02-4ed3-90f4-d4b71735a356",
   "metadata": {},
   "source": [
    "#### Retrieval Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ddd1715-8683-4893-97d5-6dab6ca2f07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradeDocuments(BaseModel):\n",
    "    binary_score: str = Field(\n",
    "        description=\"Are the documents relevant to the user query? Answer 'yes' or 'no'.\"\n",
    "    )\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature = 0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "retrieval_grader = retrieval_grader_prompt | structured_llm_grader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa1ffae-3438-4091-ad6c-1fa92b990261",
   "metadata": {},
   "source": [
    "#### Test Retrieval Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97621783-b043-43c7-8aa4-df1835f66db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "query = \"삼성전자가 만든 생성형 AI 의 이름은?\"\n",
    "retrieved_docs = pdf_retriever.invoke(query)\n",
    "retrieved_doc = retrieved_docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"query\":query, \"documents\":retrieved_doc}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f66d3956-6a24-4f5b-9c3f-dbfa14b7f75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_docs = []\n",
    "for doc in retrieved_docs:\n",
    "    result = retrieval_grader.invoke(\n",
    "        {\n",
    "            \"query\": query,\n",
    "            \"documents\": doc.page_content,\n",
    "        }\n",
    "    )\n",
    "    if result.binary_score == \"yes\":\n",
    "        filtered_docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b2abcee-947d-4e0d-8c86-fa484b9aadfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='61e5bff7-72c2-4199-a330-cbc69b255c28', metadata={'source': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'page': 1, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}, page_content='▹ 삼성전자, 자체 개발 생성 AI ‘삼성 가우스’ 공개 ···························································10\\n▹ 구글, 앤스로픽에 20억 달러 투자로 생성 AI 협력 강화 ················································11\\n▹ IDC, 2027년 AI 소프트웨어 매출 2,500억 달러 돌파 전망···········································12'), Document(id='83962b39-ff5f-494f-bd3e-ce8ca4acca0b', metadata={'source': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'page': 12, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}, page_content='SPRi AI Brief |\\n2023-12월호\\n삼성전자, 자체 개발 생성 AI ‘삼성 가우스’ 공개\\nKEY Contents\\nn 삼성전자가 온디바이스에서 작동 가능하며 언어, 코드, 이미지의 3개 모델로 구성된 자체 개발 생성\\nAI 모델 ‘삼성 가우스’를 공개\\nn 삼성전자는 삼성 가우스를 다양한 제품에 단계적으로 탑재할 계획으로, 온디바이스 작동이 가능한\\n삼성 가우스는 외부로 사용자 정보가 유출될 위험이 없다는 장점을 보유\\n£언어, 코드, 이미지의 3개 모델로 구성된 삼성 가우스, 온디바이스 작동 지원'), Document(id='3a12fda1-b1d8-4458-be87-4e38c0e574b7', metadata={'source': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'page': 12, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}, page_content='£언어, 코드, 이미지의 3개 모델로 구성된 삼성 가우스, 온디바이스 작동 지원\\nn 삼성전자가 2023년 11월 8일 열린 ‘삼성 AI 포럼 2023’ 행사에서 자체 개발한 생성 AI 모델\\n‘삼성 가우스’를 최초 공개\\n∙ 정규분포 이론을 정립한 천재 수학자 가우스(Gauss)의 이름을 본뜬 삼성 가우스는 다양한 상황에\\n최적화된 크기의 모델 선택이 가능\\n∙ 삼성 가우스는 라이선스나 개인정보를 침해하지 않는 안전한 데이터를 통해 학습되었으며,\\n온디바이스에서 작동하도록 설계되어 외부로 사용자의 정보가 유출되지 않는 장점을 보유'), Document(id='53d28e10-4c71-4b28-85c2-896b4b284f4b', metadata={'source': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'page': 12, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}, page_content='어시스턴트를 적용한 구글 픽셀(Pixel)과 경쟁할 것으로 예상\\n☞ 출처 : 삼성전자, ‘삼성 AI 포럼’서 자체 개발 생성형 AI ‘삼성 가우스’ 공개, 2023.11.08.\\n삼성전자, ‘삼성 개발자 콘퍼런스 코리아 2023’ 개최, 2023.11.14.\\nTechRepublic, Samsung Gauss: Samsung Research Reveals Generative AI, 2023.11.08.\\n10'), Document(id='fdf8c68c-0732-4230-975c-e176d50fe89b', metadata={'source': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'page': 12, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}, page_content='처리를 지원\\n∙ 코드 모델 기반의 AI 코딩 어시스턴트 ‘코드아이(code.i)’는 대화형 인터페이스로 서비스를 제공하며\\n사내 소프트웨어 개발에 최적화\\n∙ 이미지 모델은 창의적인 이미지를 생성하고 기존 이미지를 원하는 대로 바꿀 수 있도록 지원하며\\n저해상도 이미지의 고해상도 전환도 지원\\nn IT 전문지 테크리퍼블릭(TechRepublic)은 온디바이스 AI가 주요 기술 트렌드로 부상했다며,\\n2024년부터 가우스를 탑재한 삼성 스마트폰이 메타의 라마(Llama)2를 탑재한 퀄컴 기기 및 구글'), Document(id='0996d3e5-70d5-49ef-8732-0db3b68f56f8', metadata={'source': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'page': 12, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}, page_content='온디바이스에서 작동하도록 설계되어 외부로 사용자의 정보가 유출되지 않는 장점을 보유\\n∙ 삼성전자는 삼성 가우스를 활용한 온디바이스 AI 기술도 소개했으며, 생성 AI 모델을 다양한 제품에\\n단계적으로 탑재할 계획\\nn 삼성 가우스는 △텍스트를 생성하는 언어모델 △코드를 생성하는 코드 모델 △이미지를 생성하는\\n이미지 모델의 3개 모델로 구성\\n∙ 언어 모델은 클라우드와 온디바이스 대상 다양한 모델로 구성되며, 메일 작성, 문서 요약, 번역 업무의\\n처리를 지원')]\n"
     ]
    }
   ],
   "source": [
    "print(filtered_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41b3237-e648-4778-b934-bb7762484d3b",
   "metadata": {},
   "source": [
    "### RAG Chain for answer generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f0abeaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 시스템 지침 포함한 프롬프트 생성\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "     \"\"\"You are an AI assistant specializing in Question-Answering (QA) tasks within a Retrieval-Augmented Generation (RAG) system.\n",
    "\n",
    "Your job is to answer the user query using only the provided context.\n",
    "\n",
    "- Be concise and accurate.\n",
    "- If the answer is not in the context, say \"I don't know\".\n",
    "- List the source(s) if available, otherwise omit.\n",
    "\n",
    "User Query: {query}\n",
    "\n",
    "Provided Context:\n",
    "{context}\n",
    "\n",
    "Answer:\"\"\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "691fc86a-1134-464d-a029-6cd804fb3e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "query = \"삼성전자가 만든 생성형 AI 의 이름은?\"\n",
    "\n",
    "# prompt = hub.pull(\"qa-rag-prompt/general\")\n",
    "\n",
    "llm = ChatOpenAI(model_name=MODEL_NAME, temperature=0.9)\n",
    "\n",
    "def format_docs(retrieved_docs):\n",
    "    return \"\\n\\n\".join(\n",
    "        [\n",
    "            f\"\"\"<document>\n",
    "                    <content>\n",
    "                        {doc.page_content}\n",
    "                    </content>\n",
    "                    <source>\n",
    "                        {doc.metadata[\"source\"]}\n",
    "                    </source>\n",
    "                    <page>\n",
    "                        {doc.metadata[\"page\"]+1}\n",
    "                    </page>\n",
    "                </docuemnt>\"\"\" for doc in retrieved_docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([f\"{doc.page_content} (Source: {doc.metadata['source']}, Page {doc.metadata['page']+1})\" for doc in docs])\n",
    "\n",
    "\n",
    "rag_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f51564f7-857b-4c85-9377-2b7f25435cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삼성전자가 만든 생성형 AI의 이름은 '삼성 가우스'입니다.\n"
     ]
    }
   ],
   "source": [
    "answer = rag_chain.invoke({\"context\": format_docs(retrieved_docs), \"query\": query})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4840bc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'query'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'qa-rag-prompt', 'lc_hub_repo': 'general', 'lc_hub_commit_hash': 'ed61f794b1d06a1dbf28e4e20f4e7f59234aa58d65ddca2ddae4d73adb3e3dd9'} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'query'], input_types={}, partial_variables={}, template='You are an AI assistant specializing in Question-Answering (QA) tasks within a Retrieval-Augmented Generation (RAG) system. \\\\n\\\\n\\nYour primary mission is to answer queries based on the provided context or chat history. \\\\n\\\\n\\nEnsure your response is concise and directly addresses the query without any additional narration. \\\\n\\\\n\\n\\nInstructions: \\\\n\\\\n \\nCarefully read and understand the provided context. \\\\n\\\\n\\n\\nIdentify the key information relevant to the query. \\\\n\\\\n\\n\\nFormulate a concise answer based on the relevant information. \\\\n\\\\n\\n\\nEnsure your final answer directly addresses the query. \\\\n\\\\n\\n\\nList the source(s) of the answer in bullet points, using file names (with page numbers) or URLs from the context. Omit if no source is found. \\\\n\\\\n\\n\\nOutput Format: \\\\n\\\\n\\n[Your final answer here, including numerical values, technical terms, jargon, and names as originally written] \\\\n\\\\n\\n\\nSource (optional): \\\\n\\\\n\\n\\n(Source of the answer; must be a file name with page number, or a URL from the context) \\\\n\\\\n\\n\\n(List additional sources if applicable) \\\\n\\\\n\\n\\n... \\\\n\\\\n\\n\\nImportant Notes: \\\\n\\\\n\\nBase your answer solely on the provided context. \\\\n\\\\n\\n\\nDo not use external knowledge or any information not included in the materials. \\\\n\\\\n\\n\\nIf the answer is not found in the context, respond with: \"I don\\'t know.\" \\\\n\\\\n\\n\\nUser Query:\\\\n\\\\n\\n{query} \\\\n\\\\n\\n\\nProvided Context: \\\\n\\\\n\\n{context} \\\\n\\\\n\\n\\nFinal Answer: \\\\n\\\\n'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=''), additional_kwargs={})]\n",
      "client=<openai.resources.chat.completions.completions.Completions object at 0x0000029BEF0B6690> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000029BEF0B6F10> root_client=<openai.OpenAI object at 0x0000029BEF0B5E10> root_async_client=<openai.AsyncOpenAI object at 0x0000029BEF0B67A0> model_name='gpt-4o-mini' temperature=0.9 model_kwargs={} openai_api_key=SecretStr('**********')\n",
      "\n",
      "first=ChatPromptTemplate(input_variables=['context', 'query'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'qa-rag-prompt', 'lc_hub_repo': 'general', 'lc_hub_commit_hash': 'ed61f794b1d06a1dbf28e4e20f4e7f59234aa58d65ddca2ddae4d73adb3e3dd9'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'query'], input_types={}, partial_variables={}, template='You are an AI assistant specializing in Question-Answering (QA) tasks within a Retrieval-Augmented Generation (RAG) system. \\\\n\\\\n\\nYour primary mission is to answer queries based on the provided context or chat history. \\\\n\\\\n\\nEnsure your response is concise and directly addresses the query without any additional narration. \\\\n\\\\n\\n\\nInstructions: \\\\n\\\\n \\nCarefully read and understand the provided context. \\\\n\\\\n\\n\\nIdentify the key information relevant to the query. \\\\n\\\\n\\n\\nFormulate a concise answer based on the relevant information. \\\\n\\\\n\\n\\nEnsure your final answer directly addresses the query. \\\\n\\\\n\\n\\nList the source(s) of the answer in bullet points, using file names (with page numbers) or URLs from the context. Omit if no source is found. \\\\n\\\\n\\n\\nOutput Format: \\\\n\\\\n\\n[Your final answer here, including numerical values, technical terms, jargon, and names as originally written] \\\\n\\\\n\\n\\nSource (optional): \\\\n\\\\n\\n\\n(Source of the answer; must be a file name with page number, or a URL from the context) \\\\n\\\\n\\n\\n(List additional sources if applicable) \\\\n\\\\n\\n\\n... \\\\n\\\\n\\n\\nImportant Notes: \\\\n\\\\n\\nBase your answer solely on the provided context. \\\\n\\\\n\\n\\nDo not use external knowledge or any information not included in the materials. \\\\n\\\\n\\n\\nIf the answer is not found in the context, respond with: \"I don\\'t know.\" \\\\n\\\\n\\n\\nUser Query:\\\\n\\\\n\\n{query} \\\\n\\\\n\\n\\nProvided Context: \\\\n\\\\n\\n{context} \\\\n\\\\n\\n\\nFinal Answer: \\\\n\\\\n'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=''), additional_kwargs={})]) middle=[ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000029BEF0B6690>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000029BEF0B6F10>, root_client=<openai.OpenAI object at 0x0000029BEF0B5E10>, root_async_client=<openai.AsyncOpenAI object at 0x0000029BEF0B67A0>, model_name='gpt-4o-mini', temperature=0.9, model_kwargs={}, openai_api_key=SecretStr('**********'))] last=StrOutputParser()\n",
      "I don't know.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'query'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'qa-rag-prompt', 'lc_hub_repo': 'general', 'lc_hub_commit_hash': 'ed61f794b1d06a1dbf28e4e20f4e7f59234aa58d65ddca2ddae4d73adb3e3dd9'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'query'], input_types={}, partial_variables={}, template='You are an AI assistant specializing in Question-Answering (QA) tasks within a Retrieval-Augmented Generation (RAG) system. \\\\n\\\\n\\nYour primary mission is to answer queries based on the provided context or chat history. \\\\n\\\\n\\nEnsure your response is concise and directly addresses the query without any additional narration. \\\\n\\\\n\\n\\nInstructions: \\\\n\\\\n \\nCarefully read and understand the provided context. \\\\n\\\\n\\n\\nIdentify the key information relevant to the query. \\\\n\\\\n\\n\\nFormulate a concise answer based on the relevant information. \\\\n\\\\n\\n\\nEnsure your final answer directly addresses the query. \\\\n\\\\n\\n\\nList the source(s) of the answer in bullet points, using file names (with page numbers) or URLs from the context. Omit if no source is found. \\\\n\\\\n\\n\\nOutput Format: \\\\n\\\\n\\n[Your final answer here, including numerical values, technical terms, jargon, and names as originally written] \\\\n\\\\n\\n\\nSource (optional): \\\\n\\\\n\\n\\n(Source of the answer; must be a file name with page number, or a URL from the context) \\\\n\\\\n\\n\\n(List additional sources if applicable) \\\\n\\\\n\\n\\n... \\\\n\\\\n\\n\\nImportant Notes: \\\\n\\\\n\\nBase your answer solely on the provided context. \\\\n\\\\n\\n\\nDo not use external knowledge or any information not included in the materials. \\\\n\\\\n\\n\\nIf the answer is not found in the context, respond with: \"I don\\'t know.\" \\\\n\\\\n\\n\\nUser Query:\\\\n\\\\n\\n{query} \\\\n\\\\n\\n\\nProvided Context: \\\\n\\\\n\\n{context} \\\\n\\\\n\\n\\nFinal Answer: \\\\n\\\\n'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=''), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000029BEF0B6690>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000029BEF0B6F10>, root_client=<openai.OpenAI object at 0x0000029BEF0B5E10>, root_async_client=<openai.AsyncOpenAI object at 0x0000029BEF0B67A0>, model_name='gpt-4o-mini', temperature=0.9, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(prompt)\n",
    "print(llm)\n",
    "print(StrOutputParser())\n",
    "print(rag_chain)\n",
    "print(answer)\n",
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83a145b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'query'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'qa-rag-prompt', 'lc_hub_repo': 'general', 'lc_hub_commit_hash': 'ed61f794b1d06a1dbf28e4e20f4e7f59234aa58d65ddca2ddae4d73adb3e3dd9'} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'query'], input_types={}, partial_variables={}, template='You are an AI assistant specializing in Question-Answering (QA) tasks within a Retrieval-Augmented Generation (RAG) system. \\\\n\\\\n\\nYour primary mission is to answer queries based on the provided context or chat history. \\\\n\\\\n\\nEnsure your response is concise and directly addresses the query without any additional narration. \\\\n\\\\n\\n\\nInstructions: \\\\n\\\\n \\nCarefully read and understand the provided context. \\\\n\\\\n\\n\\nIdentify the key information relevant to the query. \\\\n\\\\n\\n\\nFormulate a concise answer based on the relevant information. \\\\n\\\\n\\n\\nEnsure your final answer directly addresses the query. \\\\n\\\\n\\n\\nList the source(s) of the answer in bullet points, using file names (with page numbers) or URLs from the context. Omit if no source is found. \\\\n\\\\n\\n\\nOutput Format: \\\\n\\\\n\\n[Your final answer here, including numerical values, technical terms, jargon, and names as originally written] \\\\n\\\\n\\n\\nSource (optional): \\\\n\\\\n\\n\\n(Source of the answer; must be a file name with page number, or a URL from the context) \\\\n\\\\n\\n\\n(List additional sources if applicable) \\\\n\\\\n\\n\\n... \\\\n\\\\n\\n\\nImportant Notes: \\\\n\\\\n\\nBase your answer solely on the provided context. \\\\n\\\\n\\n\\nDo not use external knowledge or any information not included in the materials. \\\\n\\\\n\\n\\nIf the answer is not found in the context, respond with: \"I don\\'t know.\" \\\\n\\\\n\\n\\nUser Query:\\\\n\\\\n\\n{query} \\\\n\\\\n\\n\\nProvided Context: \\\\n\\\\n\\n{context} \\\\n\\\\n\\n\\nFinal Answer: \\\\n\\\\n'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=''), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c6bf61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
