{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f4c3bec-f9fa-4539-8234-7071613ec3d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b3c729-3aaa-47e1-9043-a92564e5a123",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Invoking / Streaming funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce43a28-eed5-4021-ad58-5d65dbcdf015",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from typing import Any, Dict, List, Callable, Optional\n",
    "\n",
    "def stream_graph(\n",
    "    graph: CompiledStateGraph,\n",
    "    inputs: dict,\n",
    "    config: RunnableConfig,\n",
    "    node_names: List[str] = [],\n",
    "    callback: Callable = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    LangGraphì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•˜ì—¬ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
    "\n",
    "    Args:\n",
    "        graph (CompiledStateGraph): ì‹¤í–‰í•  ì»´íŒŒì¼ëœ LangGraph ê°ì²´\n",
    "        inputs (dict): ê·¸ë˜í”„ì— ì „ë‹¬í•  ì…ë ¥ê°’ ë”•ì…”ë„ˆë¦¬\n",
    "        config (RunnableConfig): ì‹¤í–‰ ì„¤ì •\n",
    "        node_names (List[str], optional): ì¶œë ¥í•  ë…¸ë“œ ì´ë¦„ ëª©ë¡. ê¸°ë³¸ê°’ì€ ë¹ˆ ë¦¬ìŠ¤íŠ¸\n",
    "        callback (Callable, optional): ê° ì²­í¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì½œë°± í•¨ìˆ˜. ê¸°ë³¸ê°’ì€ None\n",
    "            ì½œë°± í•¨ìˆ˜ëŠ” {\"node\": str, \"content\": str} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ë¥¼ ì¸ìë¡œ ë°›ìŠµë‹ˆë‹¤.\n",
    "\n",
    "    Returns:\n",
    "        None: í•¨ìˆ˜ëŠ” ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ë¥¼ ì¶œë ¥ë§Œ í•˜ê³  ë°˜í™˜ê°’ì€ ì—†ìŠµë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    prev_node = \"\"\n",
    "    for chunk_msg, metadata in graph.stream(inputs, config, stream_mode=\"messages\"):\n",
    "        curr_node = metadata[\"langgraph_node\"]\n",
    "\n",
    "        # node_namesê°€ ë¹„ì–´ìˆê±°ë‚˜ í˜„ì¬ ë…¸ë“œê°€ node_namesì— ìˆëŠ” ê²½ìš°ì—ë§Œ ì²˜ë¦¬\n",
    "        if not node_names or curr_node in node_names:\n",
    "            # ì½œë°± í•¨ìˆ˜ê°€ ìˆëŠ” ê²½ìš° ì‹¤í–‰\n",
    "            if callback:\n",
    "                callback({\"node\": curr_node, \"content\": chunk_msg.content})\n",
    "            # ì½œë°±ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì¶œë ¥\n",
    "            else:\n",
    "                # ë…¸ë“œê°€ ë³€ê²½ëœ ê²½ìš°ì—ë§Œ êµ¬ë¶„ì„  ì¶œë ¥\n",
    "                if curr_node != prev_node:\n",
    "                    print(\"\\n\" + \"=\" * 50)\n",
    "                    print(f\"ğŸ”„ Node: \\033[1;36m{curr_node}\\033[0m ğŸ”„\")\n",
    "                    print(\"- \" * 25)\n",
    "                print(chunk_msg.content, end=\"\", flush=True)\n",
    "\n",
    "            prev_node = curr_node\n",
    "\n",
    "\n",
    "def invoke_graph(\n",
    "    graph: CompiledStateGraph,\n",
    "    inputs: dict,\n",
    "    config: RunnableConfig,\n",
    "    node_names: List[str] = [],\n",
    "    callback: Callable = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    LangGraph ì•±ì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ ì˜ˆì˜ê²Œ ìŠ¤íŠ¸ë¦¬ë°í•˜ì—¬ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
    "\n",
    "    Args:\n",
    "        graph (CompiledStateGraph): ì‹¤í–‰í•  ì»´íŒŒì¼ëœ LangGraph ê°ì²´\n",
    "        inputs (dict): ê·¸ë˜í”„ì— ì „ë‹¬í•  ì…ë ¥ê°’ ë”•ì…”ë„ˆë¦¬\n",
    "        config (RunnableConfig): ì‹¤í–‰ ì„¤ì •\n",
    "        node_names (List[str], optional): ì¶œë ¥í•  ë…¸ë“œ ì´ë¦„ ëª©ë¡. ê¸°ë³¸ê°’ì€ ë¹ˆ ë¦¬ìŠ¤íŠ¸\n",
    "        callback (Callable, optional): ê° ì²­í¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì½œë°± í•¨ìˆ˜. ê¸°ë³¸ê°’ì€ None\n",
    "            ì½œë°± í•¨ìˆ˜ëŠ” {\"node\": str, \"content\": str} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ë¥¼ ì¸ìë¡œ ë°›ìŠµë‹ˆë‹¤.\n",
    "\n",
    "    Returns:\n",
    "        None: í•¨ìˆ˜ëŠ” ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ë¥¼ ì¶œë ¥ë§Œ í•˜ê³  ë°˜í™˜ê°’ì€ ì—†ìŠµë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "\n",
    "    def format_namespace(namespace):\n",
    "        return namespace[-1].split(\":\")[0] if len(namespace) > 0 else \"root graph\"\n",
    "\n",
    "    # subgraphs=True ë¥¼ í†µí•´ ì„œë¸Œê·¸ë˜í”„ì˜ ì¶œë ¥ë„ í¬í•¨\n",
    "    for namespace, chunk in graph.stream(\n",
    "        inputs, config, stream_mode=\"updates\", subgraphs=True\n",
    "    ):\n",
    "        for node_name, node_chunk in chunk.items():\n",
    "            # node_namesê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ í•„í„°ë§\n",
    "            if len(node_names) > 0 and node_name not in node_names:\n",
    "                continue\n",
    "\n",
    "            # ì½œë°± í•¨ìˆ˜ê°€ ìˆëŠ” ê²½ìš° ì‹¤í–‰\n",
    "            if callback is not None:\n",
    "                callback({\"node\": node_name, \"content\": node_chunk})\n",
    "            # ì½œë°±ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì¶œë ¥\n",
    "            else:\n",
    "                print(\"\\n\" + \"=\" * 50)\n",
    "                formatted_namespace = format_namespace(namespace)\n",
    "                if formatted_namespace == \"root graph\":\n",
    "                    print(f\"ğŸ”„ Node: \\033[1;36m{node_name}\\033[0m ğŸ”„\")\n",
    "                else:\n",
    "                    print(\n",
    "                        f\"ğŸ”„ Node: \\033[1;36m{node_name}\\033[0m in [\\033[1;33m{formatted_namespace}\\033[0m] ğŸ”„\"\n",
    "                    )\n",
    "                print(\"- \" * 25)\n",
    "\n",
    "                # ë…¸ë“œì˜ ì²­í¬ ë°ì´í„° ì¶œë ¥\n",
    "                if isinstance(node_chunk, dict):\n",
    "                    for k, v in node_chunk.items():\n",
    "                        if isinstance(v, BaseMessage):\n",
    "                            v.pretty_print()\n",
    "                        elif isinstance(v, list):\n",
    "                            for list_item in v:\n",
    "                                if isinstance(list_item, BaseMessage):\n",
    "                                    list_item.pretty_print()\n",
    "                                else:\n",
    "                                    print(list_item)\n",
    "                        elif isinstance(v, dict):\n",
    "                            for node_chunk_key, node_chunk_value in node_chunk.items():\n",
    "                                print(f\"{node_chunk_key}:\\n{node_chunk_value}\")\n",
    "                        else:\n",
    "                            print(f\"\\033[1;32m{k}\\033[0m:\\n{v}\")\n",
    "                else:\n",
    "                    if node_chunk is not None:\n",
    "                        for item in node_chunk:\n",
    "                            print(item)\n",
    "                print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b795ffa5-caaf-4ecd-af80-a8c4502ea359",
   "metadata": {},
   "source": [
    "### Doc Evaluating funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a76980f7-2b30-4955-9fd7-ebaca23eb76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class grade(BaseModel):\n",
    "    binary_score: str = Field(\n",
    "        description=\"Return 'yes' if the retrieved document is relevant to the question, otherwise 'no'.\"\n",
    "    )\n",
    "\n",
    "def grade_documents(state) -> Literal[\"generate\", \"rewrite\"]:\n",
    "    \n",
    "    model = ChatOpenAI(temperature=0, model=MODEL_NAME, streaming = True)\n",
    "    llm_with_tool = model.with_structured_output(grade)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template = \"\"\"\n",
    "        You are a grader assessing relevance of a retrieved document to a user question \\n\n",
    "        Here is the question: {question} \\n\\n\n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        If the document contains any keywords or semantic meanings related to the user question, grade it as relevant \\n\n",
    "        Give a binary score 'yes' or 'no' to indicate whether the document is relevant to the question. \"\"\",\n",
    "        input_variables = [\"question\", \"context\"],\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm_with_tool\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    last_message = messages[-1]\n",
    "    retrieved_docs = last_message.content ### ASSUME that the last message is the retrieved_docs.\n",
    "\n",
    "    question = messages[0].content\n",
    "\n",
    "    scored_result = chain.invoke({\"question\":question, \"context\":retrieved_docs})\n",
    "\n",
    "    score = scored_result.binary_score\n",
    "    \n",
    "    if score == \"yes\":\n",
    "        print(\"=== [DECISION: DOCS RELEVENT] ===\")\n",
    "        return \"generate\"\n",
    "    else:\n",
    "        print(\"=== [DECISION: DOCS IRRELEVENT] ===\")\n",
    "        return \"rewrite\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e106eb3-aca6-40de-aabb-b9a58788305a",
   "metadata": {},
   "source": [
    "### PDF Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e47f3db-54e7-4cbf-84a3-3dbce4ecf887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/RAGwithLangChain.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    }
   ],
   "source": [
    "from rag.pdf import PDFRetrievalChain\n",
    "\n",
    "# PDF ë¬¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "pdf = PDFRetrievalChain([\"data/RAGwithLangChain.pdf\"]).create_chain()\n",
    "\n",
    "# retrieverì™€ chainì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "pdf_retriever = pdf.retriever\n",
    "pdf_chain = pdf.chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "28a6321e-e3c3-46b4-a9fc-5eca4474a490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools.retriever import create_retriever_tool\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    pdf_retriever,\n",
    "    \"pdf_retriever\",\n",
    "     \"\"\"Search and return information about SPRI AI Brief PDF file. \n",
    "        It contains useful information on recent AI trends. \n",
    "        The document is published in Dec 2023.\"\"\",\n",
    "    document_prompt = PromptTemplate.from_template(\n",
    "      \"\"\"<document>\n",
    "            <content>\n",
    "                {page_content}\n",
    "            </content>\n",
    "            <metadata>\n",
    "                <source>\n",
    "                    {source}\n",
    "                </source>\n",
    "                <page>\n",
    "                    {page}\n",
    "                </page>\n",
    "            </metadata>\n",
    "        </document>\"\"\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fe991f-2f9e-45ba-8883-6bcc315aa738",
   "metadata": {},
   "source": [
    "## State Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "23892cbd-6710-4829-8e34-4a57a7ce57ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence, TypedDict\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "class AgentState(MessagesState):\n",
    "    # messages: Annotated[list, add_messages] # pre-built in MessagesState\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a58ead0-8bd2-4735-823c-98fb0e9f517a",
   "metadata": {},
   "source": [
    "## Node definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dd53a6ad-ccee-48fe-b061-148141db31fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langchain import hub\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "\n",
    "def agent(state: AgentState) -> AgentState:\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    llm = ChatOpenAI(temperature=0, streaming = True, model = MODEL_NAME)\n",
    "\n",
    "    llm_with_tools = llm.bind_tools([retriever_tool])\n",
    "\n",
    "    tool_call = llm_with_tools.invoke(messages)\n",
    "\n",
    "    return {\"messages\": [tool_call]}\n",
    "\n",
    "def rewrite(state: AgentState) -> AgentState:\n",
    "    print(\"=== [REWRITE QUERY] ===\")\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    question = messages[0].content\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(\n",
    "            content=f\"\"\"\\n\n",
    "                        Look at the input and try to reason about the underlying semantic intention or meaing \\n\n",
    "                        \\n ----------- \\n\n",
    "                        {question}\n",
    "                        \\n ----------- \\n\n",
    "                        FOrmulate an improved question: \"\"\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    model = ChatOpenAI(temperature=0, model=MODEL_NAME, streaming=True)\n",
    "    response = model.invoke(msg)\n",
    "\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def generate(state):\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    question = messages[0].content\n",
    "    docs = messages[-1].content\n",
    "\n",
    "    prompt = hub.pull(\"teddynote/rag-prompt\")\n",
    "\n",
    "    llm = ChatOpenAI(model_name=MODEL_NAME, temperature=0, streaming=True)\n",
    "\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    response = rag_chain.invoke({\"context\":docs, \"question\": question})\n",
    "\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738883ef-ef11-4025-a459-1a2b8e5283d3",
   "metadata": {},
   "source": [
    "## Graph Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "467a60bc-4621-4ee9-81a7-30b1206d48ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Add Nodes\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"agent\", agent)\n",
    "retrieve = ToolNode([retriever_tool])\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"rewrite\", rewrite)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "\n",
    "# Add Edges\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\", # from \"agent\"\n",
    "    tools_condition, # call tools_condition\n",
    "    {\n",
    "        \"tools\": \"retrieve\", # If tools_condition returns \"tools\", go to \"retrieve\"\n",
    "        \"END\": END, # If tools_condition return \"END\", go to END\n",
    "    },\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\", # from retrieve\n",
    "    grade_documents, # call grade_documents\n",
    ")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "workflow.add_edge(\"rewrite\", \"agent\")\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "42247c39-e161-4b14-9abb-af65a4524d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Visualize Graph Error: Failed to reach https://mermaid.ink/ API while trying to render your graph after 1 retries. To resolve this issue:\n",
      "1. Check your internet connection and try again\n",
      "2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n",
      "3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`\n"
     ]
    }
   ],
   "source": [
    "from utils.visualize import visualize_graph\n",
    "visualize_graph(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "43eb3d95-cfd1-4395-9b0a-5ab37c38d83d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  pdf_retriever (call_UfbEZx2yHIDAAy6LxVol2dUF)\n",
      " Call ID: call_UfbEZx2yHIDAAy6LxVol2dUF\n",
      "  Args:\n",
      "    query: RAG Langchain ì°¨ì´\n",
      "==================================================\n",
      "=== [DECISION: DOCS RELEVENT] ===\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mretrieve\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: pdf_retriever\n",
      "\n",
      "<document>\n",
      "            <content>\n",
      "                PART\n",
      "RAG with LangChain\n",
      "            </content>\n",
      "            <metadata>\n",
      "                <source>\n",
      "                    data/RAGwithLangChain.pdf\n",
      "                </source>\n",
      "                <page>\n",
      "                    1\n",
      "                </page>\n",
      "            </metadata>\n",
      "        </document>\n",
      "\n",
      "<document>\n",
      "            <content>\n",
      "                RAGë¥¼ í™œìš©í•œ ì™„ì„±ë„ ë†’ì€ LLM ì„œë¹„ìŠ¤ êµ¬ì¶•\n",
      "With Langchain & LLamaIndex\n",
      "            </content>\n",
      "            <metadata>\n",
      "                <source>\n",
      "                    data/RAGwithLangChain.pdf\n",
      "                </source>\n",
      "                <page>\n",
      "                    0\n",
      "                </page>\n",
      "            </metadata>\n",
      "        </document>\n",
      "\n",
      "<document>\n",
      "            <content>\n",
      "                CLIP\n",
      "Introduction to LangChain\n",
      "            </content>\n",
      "            <metadata>\n",
      "                <source>\n",
      "                    data/RAGwithLangChain.pdf\n",
      "                </source>\n",
      "                <page>\n",
      "                    2\n",
      "                </page>\n",
      "            </metadata>\n",
      "        </document>\n",
      "\n",
      "<document>\n",
      "            <content>\n",
      "                Introduction to LangChain\n",
      "RAG Pipeline\n",
      "User query\n",
      "2024ë…„ ìµœì €ì‹œê¸‰ì€ ì–¼ë§ˆì•¼?\n",
      "2024ë…„ í•œêµ­ì˜ ìµœì €ì„ê¸ˆì€ 9860ì›ì…ë‹ˆë‹¤.\n",
      "Indexing Retrieval Generation\n",
      "chunks Relevant ë‹¤ìŒ ì£¼ì–´ì§€ëŠ” ë¬¸ì„œë¥¼ ì°¸ê³ í•´\n",
      "documents chunk1: \"ì„ê¸ˆì€ 2024ë…„ ìµœì €\n",
      "embedding documents user questionì— ëŒ€í•œ ë‹µë³€ì„\n",
      "ì‹œê¸‰ì´ë©° êµí†µë¹„ê°€ ë³„ë„ ì§€ê¸‰ëœ\n",
      "ìƒì„±í•˜ì‹œì˜¤\n",
      "ë‹¤\"\n",
      "chunk1:\n",
      "chunk2: \"2024ë…„ 1ì›”ë¶€í„° ìµœ\n",
      "chunk2:\n",
      "            </content>\n",
      "            <metadata>\n",
      "                <source>\n",
      "                    data/RAGwithLangChain.pdf\n",
      "                </source>\n",
      "                <page>\n",
      "                    3\n",
      "                </page>\n",
      "            </metadata>\n",
      "        </document>\n",
      "\n",
      "<document>\n",
      "            <content>\n",
      "                Introduction to LangChain\n",
      "RAG Pipeline\n",
      "User query\n",
      "2024ë…„ ìµœì €ì‹œê¸‰ì€ ì–¼ë§ˆì•¼?\n",
      "2024ë…„ í•œêµ­ì˜ ìµœì €ì„ê¸ˆì€ 9860ì›ì…ë‹ˆë‹¤.\n",
      "Indexing Retrieval Generation\n",
      "chunks Relevant ë‹¤ìŒ ì£¼ì–´ì§€ëŠ” ë¬¸ì„œë¥¼ ì°¸ê³ í•´\n",
      "documents chunk1: \"ì„ê¸ˆì€ 2024ë…„ ìµœì €\n",
      "embedding documents user questionì— ëŒ€í•œ ë‹µë³€ì„\n",
      "Llaì‹œê¸‰mì´ë©° êµí†µaë¹„ê°€ ë³„Ië„n ì§€ê¸‰ëœdex\n",
      "ğŸ¦™\n",
      "ìƒì„±í•˜ì‹œì˜¤\n",
      "ë‹¤\"\n",
      "chunk1:\n",
      "chunk2: \"2024ë…„ 1ì›”ë¶€í„° ìµœ\n",
      "            </content>\n",
      "            <metadata>\n",
      "                <source>\n",
      "                    data/RAGwithLangChain.pdf\n",
      "                </source>\n",
      "                <page>\n",
      "                    4\n",
      "                </page>\n",
      "            </metadata>\n",
      "        </document>\n",
      "\n",
      "<document>\n",
      "            <content>\n",
      "                CLIP\n",
      "Comparative Analysis of RAG\n",
      "Frameworks\n",
      "            </content>\n",
      "            <metadata>\n",
      "                <source>\n",
      "                    data/RAGwithLangChain.pdf\n",
      "                </source>\n",
      "                <page>\n",
      "                    7\n",
      "                </page>\n",
      "            </metadata>\n",
      "        </document>\n",
      "\n",
      "<document>\n",
      "            <content>\n",
      "                Introduction to LangChain\n",
      "RAG Pipeline\n",
      "User query\n",
      "2024ë…„ ìµœì €ì‹œê¸‰ì€ ì–¼ë§ˆì•¼?\n",
      "2024ë…„ í•œêµ­ì˜ ìµœì €ì„ê¸ˆì€ 9860ì›ì…ë‹ˆë‹¤.\n",
      "Indexing Retrieval Generation\n",
      "chunks Relevant ë‹¤ìŒ ì£¼ì–´ì§€ëŠ” ë¬¸ì„œë¥¼ ì°¸ê³ í•´\n",
      "documents chunk1: \"ì„ê¸ˆì€ 2024ë…„ ìµœì €\n",
      "embedding documents user questionì— ëŒ€í•œ ë‹µë³€ì„\n",
      "Laì‹œê¸‰nì´ë©° êµgí†µë¹„ê°€C ë³„ë„ ì§€hê¸‰ëœ ain\n",
      "ğŸ¦œ\n",
      "ìƒì„±í•˜ì‹œì˜¤\n",
      "ë‹¤\"\n",
      "chunk1:\n",
      "chunk2: \"2024ë…„ 1ì›”ë¶€í„° ìµœ\n",
      "            </content>\n",
      "            <metadata>\n",
      "                <source>\n",
      "                    data/RAGwithLangChain.pdf\n",
      "                </source>\n",
      "                <page>\n",
      "                    5\n",
      "                </page>\n",
      "            </metadata>\n",
      "        </document>\n",
      "\n",
      "<document>\n",
      "            <content>\n",
      "                Introduction to LangChain\n",
      "Chain Vectorstore\n",
      "Document\n",
      "Memory\n",
      "loader\n",
      "ğŸ¦œğŸ”—\n",
      "Embedding\n",
      "Tool\n",
      "model\n",
      "Graph LM\n",
      "Agent\n",
      "            </content>\n",
      "            <metadata>\n",
      "                <source>\n",
      "                    data/RAGwithLangChain.pdf\n",
      "                </source>\n",
      "                <page>\n",
      "                    6\n",
      "                </page>\n",
      "            </metadata>\n",
      "        </document>\n",
      "\n",
      "<document>\n",
      "            <content>\n",
      "                Comparative analysis of RAG Frameworks\n",
      "Chain\n",
      "Vectorstore\n",
      "Memory\n",
      "Document\n",
      "loader\n",
      "ğŸ¦™\n",
      "ğŸ¦œğŸ”—\n",
      "Tool\n",
      "Embedding\n",
      "model\n",
      "Graph\n",
      "LM\n",
      "Agent\n",
      "            </content>\n",
      "            <metadata>\n",
      "                <source>\n",
      "                    data/RAGwithLangChain.pdf\n",
      "                </source>\n",
      "                <page>\n",
      "                    8\n",
      "                </page>\n",
      "            </metadata>\n",
      "        </document>\n",
      "\n",
      "<document>\n",
      "            <content>\n",
      "                Recap\n",
      "LangGraph\n",
      "            </content>\n",
      "            <metadata>\n",
      "                <source>\n",
      "                    data/RAGwithLangChain.pdf\n",
      "                </source>\n",
      "                <page>\n",
      "                    13\n",
      "                </page>\n",
      "            </metadata>\n",
      "        </document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mgenerate\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "RAGëŠ” Retrieval-Augmented Generationì˜ ì•½ìë¡œ, ì •ë³´ ê²€ìƒ‰ê³¼ ìƒì„± ëª¨ë¸ì„ ê²°í•©í•˜ì—¬ ì‚¬ìš©ì ì¿¼ë¦¬ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.\n",
      "\n",
      "**Source**\n",
      "- data/RAGwithLangChain.pdf (page 1)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langgraph.errors import GraphRecursionError\n",
    "\n",
    "config = RunnableConfig(recursion_limit=10, configurable={\"thread_id\": \"1\"})\n",
    "\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        (\"user\", \"RAGì™€ Langchainì˜ ì°¨ì´\"),\n",
    "    ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    invoke_graph(app, inputs, config)\n",
    "except GraphRecursionError as recursion_error:\n",
    "    print(f\"GraphRecursionError: {recursion_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2a8d8b9a-e43d-45ed-be0e-b41f5231e351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mretrieve\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{\"binary_score\":\"no\"}=== [DECISION: DOCS IRRELEVENT] ===\n",
      "<document>\n",
      "            <content>\n",
      "                ìƒì„±í•˜ì‹œì˜¤\n",
      "ë‹¤\"\n",
      "chunk1:\n",
      "chunk2: \"2024ë…„ 1ì›”ë¶€í„° ìµœ\n",
      "chunk2:\n",
      "ì €ì‹œê¸‰ì´ 9860ì›ìœ¼ë¡œ ì¸ìƒë˜ì—ˆ\n",
      "ë‹¤\"\n",
      "user question:\n",
      "            </content>\n",
      "            <metadata>\n",
      "                <source>\n",
      "                    data/RAGwithLangChain.pdf\n",
      "                </source>\n",
      "                <page>\n",
      "                    3\n",
      "                </page>\n",
      "            </metadata>\n",
      "        </document>\n",
      "\n",
      "<document>\n",
      "            <content>\n",
      "                ğŸ¦œ\n",
      "ìƒì„±í•˜ì‹œì˜¤\n",
      "ë‹¤\"\n",
      "chunk1:\n",
      "chunk2: \"2024ë…„ 1ì›”ë¶€í„° ìµœ\n",
      "chunk2:\n",
      "ì €ì‹œê¸‰ì´ 9860ì›ìœ¼ë¡œ ì¸ìƒë˜ì—ˆ\n",
      "ë‹¤\"\n",
      "user question:\n",
      "            </content>\n",
      "            <metadata>\n",
      "                <source>\n",
      "                    data/RAGwithLangChain.pdf\n",
      "                </source>\n",
      "                <page>\n",
      "                    5\n",
      "                </page>\n",
      "            </metadata>\n",
      "        </document>\n",
      "\n",
      "<document>\n",
      "            <content>\n",
      "                ğŸ¦™\n",
      "ìƒì„±í•˜ì‹œì˜¤\n",
      "ë‹¤\"\n",
      "chunk1:\n",
      "chunk2: \"2024ë…„ 1ì›”ë¶€í„° ìµœ\n",
      "chunk2:\n",
      "ì €ì‹œê¸‰ì´ 9860ì›ìœ¼ë¡œ ì¸ìƒë˜ì—ˆ\n",
      "ë‹¤\"\n",
      "user question:\n",
      "            </content>\n",
      "            <metadata>\n",
      "                <source>\n",
      "                    data/RAGwithLangChain.pdf\n",
      "                </source>\n",
      "                <page>\n",
      "                    4\n",
      "                </page>\n",
      "            </metadata>\n",
      "        </document>\n",
      "\n",
      "<document>\n",
      "            <content>\n",
      "                Recap\n",
      "Ollama\n",
      "            </content>\n",
      "            <metadata>\n",
      "                <source>\n",
      "                    data/RAGwithLangChain.pdf\n",
      "                </source>\n",
      "                <page>\n",
      "                    12\n",
      "                </page>\n",
      "            </metadata>\n",
      "        </document>\n",
      "\n",
      "<document>\n",
      "            <content>\n",
      "                CLIP\n",
      "Recap\n",
      "            </content>\n",
      "            <metadata>\n",
      "                <source>\n",
      "                    data/RAGwithLangChain.pdf\n",
      "                </source>\n",
      "                <page>\n",
      "                    9\n",
      "                </page>\n",
      "            </metadata>\n",
      "        </document>\n",
      "\n",
      "<document>\n",
      "            <content>\n",
      "                ì‹œê¸‰ì´ë©° êµí†µë¹„ê°€ ë³„ë„ ì§€ê¸‰ëœ\n",
      "ğŸ¦œğŸ”—\n",
      "ìƒì„±í•˜ì‹œì˜¤\n",
      "ë‹¤\"\n",
      "chunk1:\n",
      "chunk2: \"2024ë…„ 1ì›”ë¶€í„° ìµœ\n",
      "chunk2:\n",
      "ì €ì‹œê¸‰ì´ 9860ì›ìœ¼ë¡œ ì¸ìƒë˜ì—ˆ\n",
      "ë‹¤\"\n",
      "top_k=10 user question:\n",
      "mode=compact\n",
      "            </content>\n",
      "            <metadata>\n",
      "                <source>\n",
      "                    data/RAGwithLangChain.pdf\n",
      "                </source>\n",
      "                <page>\n",
      "                    10\n",
      "                </page>\n",
      "            </metadata>\n",
      "        </document>\n",
      "\n",
      "<document>\n",
      "            <content>\n",
      "                Recap\n",
      "RAG Pipeline\n",
      "User query\n",
      "2024ë…„ ìµœì €ì‹œê¸‰ì€ ì–¼ë§ˆì•¼?\n",
      "2024ë…„ í•œêµ­ì˜ ìµœì €ì„ê¸ˆì€ 9860ì›ì…ë‹ˆë‹¤.\n",
      "prompt=\"given\n",
      "embed_model=bge\n",
      "chunk_size=128 context...\"\n",
      "llm_model=mistral\n",
      "Indexing Retrieval Generation\n",
      "chunks Relevant ë‹¤ìŒ ì£¼ì–´ì§€ëŠ” ë¬¸ì„œë¥¼ ì°¸ê³ í•´\n",
      "documents chunk1: \"ì„ê¸ˆì€ 2024ë…„ ìµœì €\n",
      "embedding documents user questionì— ëŒ€í•œ ë‹µë³€ì„\n",
      "ì‹œê¸‰ì´ë©° êµí†µë¹„ê°€ ë³„ë„ ì§€ê¸‰ëœ\n",
      "            </content>\n",
      "            <metadata>\n",
      "                <source>\n",
      "                    data/RAGwithLangChain.pdf\n",
      "                </source>\n",
      "                <page>\n",
      "                    10\n",
      "                </page>\n",
      "            </metadata>\n",
      "        </document>\n",
      "\n",
      "<document>\n",
      "            <content>\n",
      "                Introduction to LangChain\n",
      "RAG Pipeline\n",
      "User query\n",
      "2024ë…„ ìµœì €ì‹œê¸‰ì€ ì–¼ë§ˆì•¼?\n",
      "2024ë…„ í•œêµ­ì˜ ìµœì €ì„ê¸ˆì€ 9860ì›ì…ë‹ˆë‹¤.\n",
      "Indexing Retrieval Generation\n",
      "chunks Relevant ë‹¤ìŒ ì£¼ì–´ì§€ëŠ” ë¬¸ì„œë¥¼ ì°¸ê³ í•´\n",
      "documents chunk1: \"ì„ê¸ˆì€ 2024ë…„ ìµœì €\n",
      "embedding documents user questionì— ëŒ€í•œ ë‹µë³€ì„\n",
      "Llaì‹œê¸‰mì´ë©° êµí†µaë¹„ê°€ ë³„Ië„n ì§€ê¸‰ëœdex\n",
      "ğŸ¦™\n",
      "ìƒì„±í•˜ì‹œì˜¤\n",
      "ë‹¤\"\n",
      "chunk1:\n",
      "chunk2: \"2024ë…„ 1ì›”ë¶€í„° ìµœ\n",
      "            </content>\n",
      "            <metadata>\n",
      "                <source>\n",
      "                    data/RAGwithLangChain.pdf\n",
      "                </source>\n",
      "                <page>\n",
      "                    4\n",
      "                </page>\n",
      "            </metadata>\n",
      "        </document>\n",
      "\n",
      "<document>\n",
      "            <content>\n",
      "                Recap\n",
      "Chain\n",
      "Vectorstore\n",
      "Memory\n",
      "Document\n",
      "loader\n",
      "ğŸ¦™\n",
      "ğŸ¦œğŸ”—\n",
      "Tool\n",
      "Embedding\n",
      "model\n",
      "Graph\n",
      "LM\n",
      "Agent\n",
      "            </content>\n",
      "            <metadata>\n",
      "                <source>\n",
      "                    data/RAGwithLangChain.pdf\n",
      "                </source>\n",
      "                <page>\n",
      "                    11\n",
      "                </page>\n",
      "            </metadata>\n",
      "        </document>\n",
      "\n",
      "<document>\n",
      "            <content>\n",
      "                CLIP\n",
      "Comparative Analysis of RAG\n",
      "Frameworks\n",
      "            </content>\n",
      "            <metadata>\n",
      "                <source>\n",
      "                    data/RAGwithLangChain.pdf\n",
      "                </source>\n",
      "                <page>\n",
      "                    7\n",
      "                </page>\n",
      "            </metadata>\n",
      "        </document>=== [REWRITE QUERY] ===\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mrewrite\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "To improve the question while maintaining its original intent, you could ask:\n",
      "\n",
      "\"ì´ì˜ì§„ì€ ì–´ë–¤ ì§ì—…ì„ ê°€ì§„ ì‚¬ëŒì¸ê°€ìš”?\" \n",
      "\n",
      "This translates to \"What is the profession of Lee Young-jin?\" which provides a clearer focus on the person's occupation or role.\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "ì´ì˜ì§„ì— ëŒ€í•œ ì •ë³´ëŠ” ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì´ì˜ì§„ì´ ì–´ë–¤ ì‚¬ëŒì¸ì§€ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì •ë³´ê°€ í•„ìš”í•˜ë‹¤ë©´, ê·¸ê°€ ì–´ë–¤ ë¶„ì•¼ì—ì„œ í™œë™í•˜ëŠ”ì§€, ë˜ëŠ” ì–´ë–¤ ì—…ì ì´ ìˆëŠ”ì§€ì— ëŒ€í•œ ì¶”ê°€ì ì¸ ë§¥ë½ì„ ì œê³µí•´ ì£¼ì‹œë©´ ë” ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'__end__'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m inputs = {\n\u001b[32m      2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m      3\u001b[39m         (\u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mì´ì˜ì§„ì€ ë­í•˜ëŠ”ì‚¬ëŒì´ì•¼?\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      4\u001b[39m     ]\n\u001b[32m      5\u001b[39m }\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[43mstream_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43magent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mretrieve\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrewrite\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgenerate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m GraphRecursionError \u001b[38;5;28;01mas\u001b[39;00m recursion_error:\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGraphRecursionError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecursion_error\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mstream_graph\u001b[39m\u001b[34m(graph, inputs, config, node_names, callback)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[33;03mLangGraphì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•˜ì—¬ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\u001b[39;00m\n\u001b[32m     13\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m \u001b[33;03m    None: í•¨ìˆ˜ëŠ” ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ë¥¼ ì¶œë ¥ë§Œ í•˜ê³  ë°˜í™˜ê°’ì€ ì—†ìŠµë‹ˆë‹¤.\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     25\u001b[39m prev_node = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk_msg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcurr_node\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlanggraph_node\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# node_namesê°€ ë¹„ì–´ìˆê±°ë‚˜ í˜„ì¬ ë…¸ë“œê°€ node_namesì— ìˆëŠ” ê²½ìš°ì—ë§Œ ì²˜ë¦¬\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Langgraph-handson\\lg-handson-env\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2461\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2455\u001b[39m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[32m   2456\u001b[39m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[32m   2457\u001b[39m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[32m   2458\u001b[39m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[32m   2459\u001b[39m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[32m   2460\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m loop.tick(input_keys=\u001b[38;5;28mself\u001b[39m.input_channels):\n\u001b[32m-> \u001b[39m\u001b[32m2461\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2462\u001b[39m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2463\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2464\u001b[39m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2465\u001b[39m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2466\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2467\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2468\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2469\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Langgraph-handson\\lg-handson-env\\Lib\\site-packages\\langgraph\\pregel\\runner.py:247\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[39m\n\u001b[32m    245\u001b[39m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m.\u001b[49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpanic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    252\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tb := exc.__traceback__:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Langgraph-handson\\lg-handson-env\\Lib\\site-packages\\langgraph\\pregel\\runner.py:499\u001b[39m, in \u001b[36m_panic_or_proceed\u001b[39m\u001b[34m(futs, timeout_exc_cls, panic)\u001b[39m\n\u001b[32m    497\u001b[39m                 interrupts.append(exc)\n\u001b[32m    498\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m499\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    500\u001b[39m \u001b[38;5;66;03m# raise combined interrupts\u001b[39;00m\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m interrupts:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Langgraph-handson\\lg-handson-env\\Lib\\site-packages\\langgraph\\pregel\\executor.py:80\u001b[39m, in \u001b[36mBackgroundExecutor.done\u001b[39m\u001b[34m(self, task)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Remove the task from the tasks dict when it's done.\"\"\"\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m GraphBubbleUp:\n\u001b[32m     82\u001b[39m     \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[32m     83\u001b[39m     \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[32m     84\u001b[39m     \u001b[38;5;28mself\u001b[39m.tasks.pop(task)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.1008.0_x64__qbz5n2kfra8p0\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.1008.0_x64__qbz5n2kfra8p0\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.1008.0_x64__qbz5n2kfra8p0\\Lib\\concurrent\\futures\\thread.py:59\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Langgraph-handson\\lg-handson-env\\Lib\\site-packages\\langgraph\\pregel\\retry.py:40\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     38\u001b[39m     task.writes.clear()\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     42\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Langgraph-handson\\lg-handson-env\\Lib\\site-packages\\langgraph\\utils\\runnable.py:625\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    623\u001b[39m                 \u001b[38;5;28minput\u001b[39m = context.run(step.invoke, \u001b[38;5;28minput\u001b[39m, config, **kwargs)\n\u001b[32m    624\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m             \u001b[38;5;28minput\u001b[39m = \u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Langgraph-handson\\lg-handson-env\\Lib\\site-packages\\langgraph\\utils\\runnable.py:377\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    375\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Langgraph-handson\\lg-handson-env\\Lib\\site-packages\\langgraph\\graph\\branch.py:174\u001b[39m, in \u001b[36mBranch._route\u001b[39m\u001b[34m(self, input, config, reader, writer)\u001b[39m\n\u001b[32m    172\u001b[39m     value = \u001b[38;5;28minput\u001b[39m\n\u001b[32m    173\u001b[39m result = \u001b[38;5;28mself\u001b[39m.path.invoke(value, config)\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Langgraph-handson\\lg-handson-env\\Lib\\site-packages\\langgraph\\graph\\branch.py:210\u001b[39m, in \u001b[36mBranch._finish\u001b[39m\u001b[34m(self, writer, input, result, config)\u001b[39m\n\u001b[32m    207\u001b[39m     result = [result]\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ends:\n\u001b[32m    209\u001b[39m     destinations: Sequence[Union[Send, \u001b[38;5;28mstr\u001b[39m]] = [\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m         r \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(r, Send) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mends\u001b[49m\u001b[43m[\u001b[49m\u001b[43mr\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m result\n\u001b[32m    211\u001b[39m     ]\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    213\u001b[39m     destinations = cast(Sequence[Union[Send, \u001b[38;5;28mstr\u001b[39m]], result)\n",
      "\u001b[31mKeyError\u001b[39m: '__end__'",
      "During task with name 'agent' and id 'f08f566a-e438-6cba-4831-7de0a0e783b5'"
     ]
    }
   ],
   "source": [
    "inputs = {\n",
    "    \"messages\": [\n",
    "        (\"user\", \"ì´ì˜ì§„ì€ ë­í•˜ëŠ”ì‚¬ëŒì´ì•¼?\"),\n",
    "    ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    stream_graph(app, inputs, config, [\"agent\", \"retrieve\", \"rewrite\", \"generate\"])\n",
    "    \n",
    "except GraphRecursionError as recursion_error:\n",
    "    print(f\"GraphRecursionError: {recursion_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cdb598b4-c743-4aa0-8985-b597763acc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: {'agent': {'messages': [AIMessage(content='RAGëŠ” Retrieval-Augmented Generationì˜ ì•½ìë¡œ, ì •ë³´ ê²€ìƒ‰ê³¼ ìƒì„± ëª¨ë¸ì„ ê²°í•©í•˜ì—¬ ì‚¬ìš©ì ì¿¼ë¦¬ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤. ì´ ì‹œìŠ¤í…œì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìì—°ì–´ë¡œ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤. \\n\\n**ì¶œì²˜**\\n- data/RAGwithLangChain.pdf (í˜ì´ì§€ 10)', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0392822090', 'service_tier': 'default'}, id='run--1d6647b5-10c1-46e8-825c-8a7ac205a39f-0')]}}\n",
      "Node: agent\n"
     ]
    }
   ],
   "source": [
    "for step in app.stream(user_input, config=config):\n",
    "    print(\"step:\", step)\n",
    "    for node_name, output in step.items():\n",
    "        print(\"Node:\", node_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce7a0c1-31de-4dcd-ba53-c8e876d709b5",
   "metadata": {},
   "source": [
    "### Where RAG is unnecessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d763994-b35f-4452-9161-7c4d99401130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì…ë‹ˆë‹¤."
     ]
    }
   ],
   "source": [
    "# ë¬¸ì„œ ê²€ìƒ‰ì´ ë¶ˆê°€ëŠ¥í•œ ì§ˆë¬¸ ì˜ˆì‹œ\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        (\"user\", \"ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ”?\"),\n",
    "    ]\n",
    "}\n",
    "try:\n",
    "    stream_graph(app, inputs, config, [\"agent\", \"rewrite\", \"generate\"])\n",
    "except GraphRecursionError as recursion_error:\n",
    "    print(f\"GraphRecursionError: {recursion_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0066e889-412c-4a94-b6f0-d1b2746dd5d5",
   "metadata": {},
   "source": [
    "### Where it is impossible to get info from the docs using RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3cfe02cf-adb5-4ad7-a682-f0157f3bfcb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "ì£„ì†¡í•˜ì§€ë§Œ, í˜„ì¬ë¡œì„œëŠ” ê·€í•˜ì˜ ì´ë¦„ì„ ì•Œ ìˆ˜ ìˆëŠ” ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ì´ë¦„ì„ ì•Œë ¤ì£¼ì‹œë©´ ê·¸ì— ë§ì¶° ëŒ€í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!"
     ]
    }
   ],
   "source": [
    "# ë¬¸ì„œ ê²€ìƒ‰ì´ ë¶ˆê°€ëŠ¥í•œ ì§ˆë¬¸ ì˜ˆì‹œ\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        (\"user\", \"ë‚´ ì´ë¦„ì„ ì•Œë ¤ì¤˜\"),\n",
    "    ]\n",
    "}\n",
    "try:\n",
    "    stream_graph(app, inputs, config, [\"agent\", \"rewrite\", \"generate\"])\n",
    "except GraphRecursionError as recursion_error:\n",
    "    print(f\"GraphRecursionError: {recursion_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4b7c6a-6f73-4f84-abf9-5ee3f2c9143a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
