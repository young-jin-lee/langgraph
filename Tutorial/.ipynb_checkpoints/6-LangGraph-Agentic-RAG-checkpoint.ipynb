{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f4c3bec-f9fa-4539-8234-7071613ec3d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b3c729-3aaa-47e1-9043-a92564e5a123",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Invoking / Streaming funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce43a28-eed5-4021-ad58-5d65dbcdf015",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from typing import Any, Dict, List, Callable, Optional\n",
    "\n",
    "def stream_graph(\n",
    "    graph: CompiledStateGraph,\n",
    "    inputs: dict,\n",
    "    config: RunnableConfig,\n",
    "    node_names: List[str] = [],\n",
    "    callback: Callable = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    LangGraph의 실행 결과를 스트리밍하여 출력하는 함수입니다.\n",
    "\n",
    "    Args:\n",
    "        graph (CompiledStateGraph): 실행할 컴파일된 LangGraph 객체\n",
    "        inputs (dict): 그래프에 전달할 입력값 딕셔너리\n",
    "        config (RunnableConfig): 실행 설정\n",
    "        node_names (List[str], optional): 출력할 노드 이름 목록. 기본값은 빈 리스트\n",
    "        callback (Callable, optional): 각 청크 처리를 위한 콜백 함수. 기본값은 None\n",
    "            콜백 함수는 {\"node\": str, \"content\": str} 형태의 딕셔너리를 인자로 받습니다.\n",
    "\n",
    "    Returns:\n",
    "        None: 함수는 스트리밍 결과를 출력만 하고 반환값은 없습니다.\n",
    "    \"\"\"\n",
    "    prev_node = \"\"\n",
    "    for chunk_msg, metadata in graph.stream(inputs, config, stream_mode=\"messages\"):\n",
    "        curr_node = metadata[\"langgraph_node\"]\n",
    "\n",
    "        # node_names가 비어있거나 현재 노드가 node_names에 있는 경우에만 처리\n",
    "        if not node_names or curr_node in node_names:\n",
    "            # 콜백 함수가 있는 경우 실행\n",
    "            if callback:\n",
    "                callback({\"node\": curr_node, \"content\": chunk_msg.content})\n",
    "            # 콜백이 없는 경우 기본 출력\n",
    "            else:\n",
    "                # 노드가 변경된 경우에만 구분선 출력\n",
    "                if curr_node != prev_node:\n",
    "                    print(\"\\n\" + \"=\" * 50)\n",
    "                    print(f\"🔄 Node: \\033[1;36m{curr_node}\\033[0m 🔄\")\n",
    "                    print(\"- \" * 25)\n",
    "                print(chunk_msg.content, end=\"\", flush=True)\n",
    "\n",
    "            prev_node = curr_node\n",
    "\n",
    "\n",
    "def invoke_graph(\n",
    "    graph: CompiledStateGraph,\n",
    "    inputs: dict,\n",
    "    config: RunnableConfig,\n",
    "    node_names: List[str] = [],\n",
    "    callback: Callable = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    LangGraph 앱의 실행 결과를 예쁘게 스트리밍하여 출력하는 함수입니다.\n",
    "\n",
    "    Args:\n",
    "        graph (CompiledStateGraph): 실행할 컴파일된 LangGraph 객체\n",
    "        inputs (dict): 그래프에 전달할 입력값 딕셔너리\n",
    "        config (RunnableConfig): 실행 설정\n",
    "        node_names (List[str], optional): 출력할 노드 이름 목록. 기본값은 빈 리스트\n",
    "        callback (Callable, optional): 각 청크 처리를 위한 콜백 함수. 기본값은 None\n",
    "            콜백 함수는 {\"node\": str, \"content\": str} 형태의 딕셔너리를 인자로 받습니다.\n",
    "\n",
    "    Returns:\n",
    "        None: 함수는 스트리밍 결과를 출력만 하고 반환값은 없습니다.\n",
    "    \"\"\"\n",
    "\n",
    "    def format_namespace(namespace):\n",
    "        return namespace[-1].split(\":\")[0] if len(namespace) > 0 else \"root graph\"\n",
    "\n",
    "    # subgraphs=True 를 통해 서브그래프의 출력도 포함\n",
    "    for namespace, chunk in graph.stream(\n",
    "        inputs, config, stream_mode=\"updates\", subgraphs=True\n",
    "    ):\n",
    "        for node_name, node_chunk in chunk.items():\n",
    "            # node_names가 비어있지 않은 경우에만 필터링\n",
    "            if len(node_names) > 0 and node_name not in node_names:\n",
    "                continue\n",
    "\n",
    "            # 콜백 함수가 있는 경우 실행\n",
    "            if callback is not None:\n",
    "                callback({\"node\": node_name, \"content\": node_chunk})\n",
    "            # 콜백이 없는 경우 기본 출력\n",
    "            else:\n",
    "                print(\"\\n\" + \"=\" * 50)\n",
    "                formatted_namespace = format_namespace(namespace)\n",
    "                if formatted_namespace == \"root graph\":\n",
    "                    print(f\"🔄 Node: \\033[1;36m{node_name}\\033[0m 🔄\")\n",
    "                else:\n",
    "                    print(\n",
    "                        f\"🔄 Node: \\033[1;36m{node_name}\\033[0m in [\\033[1;33m{formatted_namespace}\\033[0m] 🔄\"\n",
    "                    )\n",
    "                print(\"- \" * 25)\n",
    "\n",
    "                # 노드의 청크 데이터 출력\n",
    "                if isinstance(node_chunk, dict):\n",
    "                    for k, v in node_chunk.items():\n",
    "                        if isinstance(v, BaseMessage):\n",
    "                            v.pretty_print()\n",
    "                        elif isinstance(v, list):\n",
    "                            for list_item in v:\n",
    "                                if isinstance(list_item, BaseMessage):\n",
    "                                    list_item.pretty_print()\n",
    "                                else:\n",
    "                                    print(list_item)\n",
    "                        elif isinstance(v, dict):\n",
    "                            for node_chunk_key, node_chunk_value in node_chunk.items():\n",
    "                                print(f\"{node_chunk_key}:\\n{node_chunk_value}\")\n",
    "                        else:\n",
    "                            print(f\"\\033[1;32m{k}\\033[0m:\\n{v}\")\n",
    "                else:\n",
    "                    if node_chunk is not None:\n",
    "                        for item in node_chunk:\n",
    "                            print(item)\n",
    "                print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b795ffa5-caaf-4ecd-af80-a8c4502ea359",
   "metadata": {},
   "source": [
    "### Doc Evaluating funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a76980f7-2b30-4955-9fd7-ebaca23eb76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class grade(BaseModel):\n",
    "    binary_score: str = Field(\n",
    "        description=\"Return 'yes' if the retrieved document is relevant to the question, otherwise 'no'.\"\n",
    "    )\n",
    "\n",
    "def grade_documents(state) -> Literal[\"generate\", \"rewrite\"]:\n",
    "    \n",
    "    model = ChatOpenAI(temperature=0, model=MODEL_NAME, streaming = True)\n",
    "    llm_with_tool = model.with_structured_output(grade)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template = \"\"\"\n",
    "        You are a grader assessing relevance of a retrieved document to a user question \\n\n",
    "        Here is the question: {question} \\n\\n\n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        If the document contains any keywords or semantic meanings related to the user question, grade it as relevant \\n\n",
    "        Give a binary score 'yes' or 'no' to indicate whether the document is relevant to the question. \"\"\",\n",
    "        input_variables = [\"question\", \"context\"],\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm_with_tool\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    last_message = messages[-1]\n",
    "    retrieved_docs = last_message.content ### ASSUME that the last message is the retrieved_docs.\n",
    "\n",
    "    question = messages[0].content\n",
    "\n",
    "    scored_result = chain.invoke({\"question\":question, \"context\":retrieved_docs})\n",
    "\n",
    "    score = scored_result.binary_score\n",
    "    \n",
    "    if score == \"yes\":\n",
    "        print(\"=== [DECISION: DOCS RELEVENT] ===\")\n",
    "        return \"generate\"\n",
    "    else:\n",
    "        print(\"=== [DECISION: DOCS IRRELEVENT] ===\")\n",
    "        return \"rewrite\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e106eb3-aca6-40de-aabb-b9a58788305a",
   "metadata": {},
   "source": [
    "### PDF Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e47f3db-54e7-4cbf-84a3-3dbce4ecf887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/RAGwithLangChain.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    }
   ],
   "source": [
    "from rag.pdf import PDFRetrievalChain\n",
    "\n",
    "# PDF 문서를 로드합니다.\n",
    "pdf = PDFRetrievalChain([\"data/RAGwithLangChain.pdf\"]).create_chain()\n",
    "\n",
    "# retriever와 chain을 생성합니다.\n",
    "pdf_retriever = pdf.retriever\n",
    "pdf_chain = pdf.chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "28a6321e-e3c3-46b4-a9fc-5eca4474a490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools.retriever import create_retriever_tool\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    pdf_retriever,\n",
    "    \"pdf_retriever\",\n",
    "     \"\"\"Search and return information about SPRI AI Brief PDF file. \n",
    "        It contains useful information on recent AI trends. \n",
    "        The document is published in Dec 2023.\"\"\",\n",
    "    document_prompt = PromptTemplate.from_template(\n",
    "      \"\"\"<document>\n",
    "            <content>\n",
    "                {page_content}\n",
    "            </content>\n",
    "            <metadata>\n",
    "                <source>\n",
    "                    {source}\n",
    "                </source>\n",
    "                <page>\n",
    "                    {page}\n",
    "                </page>\n",
    "            </metadata>\n",
    "        </document>\"\"\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fe991f-2f9e-45ba-8883-6bcc315aa738",
   "metadata": {},
   "source": [
    "## State Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "23892cbd-6710-4829-8e34-4a57a7ce57ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence, TypedDict\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "class AgentState(MessagesState):\n",
    "    # messages: Annotated[list, add_messages] # pre-built in MessagesState\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a58ead0-8bd2-4735-823c-98fb0e9f517a",
   "metadata": {},
   "source": [
    "## Node definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dd53a6ad-ccee-48fe-b061-148141db31fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langchain import hub\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "\n",
    "def agent(state: AgentState) -> AgentState:\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    llm = ChatOpenAI(temperature=0, streaming = True, model = MODEL_NAME)\n",
    "\n",
    "    llm_with_tools = llm.bind_tools([retriever_tool])\n",
    "\n",
    "    tool_call = llm_with_tools.invoke(messages)\n",
    "\n",
    "    return {\"messages\": [tool_call]}\n",
    "\n",
    "def rewrite(state: AgentState) -> AgentState:\n",
    "    print(\"=== [REWRITE QUERY] ===\")\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    question = messages[0].content\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(\n",
    "            content=f\"\"\"\\n\n",
    "                        Look at the input and try to reason about the underlying semantic intention or meaing \\n\n",
    "                        \\n ----------- \\n\n",
    "                        {question}\n",
    "                        \\n ----------- \\n\n",
    "                        FOrmulate an improved question: \"\"\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    model = ChatOpenAI(temperature=0, model=MODEL_NAME, streaming=True)\n",
    "    response = model.invoke(msg)\n",
    "\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def generate(state):\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    question = messages[0].content\n",
    "    docs = messages[-1].content\n",
    "\n",
    "    prompt = hub.pull(\"teddynote/rag-prompt\")\n",
    "\n",
    "    llm = ChatOpenAI(model_name=MODEL_NAME, temperature=0, streaming=True)\n",
    "\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    response = rag_chain.invoke({\"context\":docs, \"question\": question})\n",
    "\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738883ef-ef11-4025-a459-1a2b8e5283d3",
   "metadata": {},
   "source": [
    "## Graph Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "467a60bc-4621-4ee9-81a7-30b1206d48ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Add Nodes\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"agent\", agent)\n",
    "retrieve = ToolNode([retriever_tool])\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"rewrite\", rewrite)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "\n",
    "# Add Edges\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\", # from \"agent\"\n",
    "    tools_condition, # call tools_condition\n",
    "    {\n",
    "        \"tools\": \"retrieve\", # If tools_condition returns \"tools\", go to \"retrieve\"\n",
    "        \"END\": END, # If tools_condition return \"END\", go to END\n",
    "    },\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\", # from retrieve\n",
    "    grade_documents, # call grade_documents\n",
    ")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "workflow.add_edge(\"rewrite\", \"agent\")\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "42247c39-e161-4b14-9abb-af65a4524d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Visualize Graph Error: Failed to reach https://mermaid.ink/ API while trying to render your graph after 1 retries. To resolve this issue:\n",
      "1. Check your internet connection and try again\n",
      "2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n",
      "3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`\n"
     ]
    }
   ],
   "source": [
    "from utils.visualize import visualize_graph\n",
    "visualize_graph(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "43eb3d95-cfd1-4395-9b0a-5ab37c38d83d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36magent\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  pdf_retriever (call_UfbEZx2yHIDAAy6LxVol2dUF)\n",
      " Call ID: call_UfbEZx2yHIDAAy6LxVol2dUF\n",
      "  Args:\n",
      "    query: RAG Langchain 차이\n",
      "==================================================\n",
      "=== [DECISION: DOCS RELEVENT] ===\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mretrieve\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: pdf_retriever\n",
      "\n",
      "<document>\n",
      "            <content>\n",
      "                PART\n",
      "RAG with LangChain\n",
      "            </content>\n",
      "            <metadata>\n",
      "                <source>\n",
      "                    data/RAGwithLangChain.pdf\n",
      "                </source>\n",
      "                <page>\n",
      "                    1\n",
      "                </page>\n",
      "            </metadata>\n",
      "        </document>\n",
      "\n",
      "<document>\n",
      "            <content>\n",
      "                RAG를 활용한 완성도 높은 LLM 서비스 구축\n",
      "With Langchain & LLamaIndex\n",
      "            </content>\n",
      "            <metadata>\n",
      "                <source>\n",
      "                    data/RAGwithLangChain.pdf\n",
      "                </source>\n",
      "                <page>\n",
      "                    0\n",
      "                </page>\n",
      "            </metadata>\n",
      "        </document>\n",
      "\n",
      "<document>\n",
      "            <content>\n",
      "                CLIP\n",
      "Introduction to LangChain\n",
      "            </content>\n",
      "            <metadata>\n",
      "                <source>\n",
      "                    data/RAGwithLangChain.pdf\n",
      "                </source>\n",
      "                <page>\n",
      "                    2\n",
      "                </page>\n",
      "            </metadata>\n",
      "        </document>\n",
      "\n",
      "<document>\n",
      "            <content>\n",
      "                Introduction to LangChain\n",
      "RAG Pipeline\n",
      "User query\n",
      "2024년 최저시급은 얼마야?\n",
      "2024년 한국의 최저임금은 9860원입니다.\n",
      "Indexing Retrieval Generation\n",
      "chunks Relevant 다음 주어지는 문서를 참고해\n",
      "documents chunk1: \"임금은 2024년 최저\n",
      "embedding documents user question에 대한 답변을\n",
      "시급이며 교통비가 별도 지급된\n",
      "생성하시오\n",
      "다\"\n",
      "chunk1:\n",
      "chunk2: \"2024년 1월부터 최\n",
      "chunk2:\n",
      "            </content>\n",
      "            <metadata>\n",
      "                <source>\n",
      "                    data/RAGwithLangChain.pdf\n",
      "                </source>\n",
      "                <page>\n",
      "                    3\n",
      "                </page>\n",
      "            </metadata>\n",
      "        </document>\n",
      "\n",
      "<document>\n",
      "            <content>\n",
      "                Introduction to LangChain\n",
      "RAG Pipeline\n",
      "User query\n",
      "2024년 최저시급은 얼마야?\n",
      "2024년 한국의 최저임금은 9860원입니다.\n",
      "Indexing Retrieval Generation\n",
      "chunks Relevant 다음 주어지는 문서를 참고해\n",
      "documents chunk1: \"임금은 2024년 최저\n",
      "embedding documents user question에 대한 답변을\n",
      "Lla시급m이며 교통a비가 별I도n 지급된dex\n",
      "🦙\n",
      "생성하시오\n",
      "다\"\n",
      "chunk1:\n",
      "chunk2: \"2024년 1월부터 최\n",
      "            </content>\n",
      "            <metadata>\n",
      "                <source>\n",
      "                    data/RAGwithLangChain.pdf\n",
      "                </source>\n",
      "                <page>\n",
      "                    4\n",
      "                </page>\n",
      "            </metadata>\n",
      "        </document>\n",
      "\n",
      "<document>\n",
      "            <content>\n",
      "                CLIP\n",
      "Comparative Analysis of RAG\n",
      "Frameworks\n",
      "            </content>\n",
      "            <metadata>\n",
      "                <source>\n",
      "                    data/RAGwithLangChain.pdf\n",
      "                </source>\n",
      "                <page>\n",
      "                    7\n",
      "                </page>\n",
      "            </metadata>\n",
      "        </document>\n",
      "\n",
      "<document>\n",
      "            <content>\n",
      "                Introduction to LangChain\n",
      "RAG Pipeline\n",
      "User query\n",
      "2024년 최저시급은 얼마야?\n",
      "2024년 한국의 최저임금은 9860원입니다.\n",
      "Indexing Retrieval Generation\n",
      "chunks Relevant 다음 주어지는 문서를 참고해\n",
      "documents chunk1: \"임금은 2024년 최저\n",
      "embedding documents user question에 대한 답변을\n",
      "La시급n이며 교g통비가C 별도 지h급된 ain\n",
      "🦜\n",
      "생성하시오\n",
      "다\"\n",
      "chunk1:\n",
      "chunk2: \"2024년 1월부터 최\n",
      "            </content>\n",
      "            <metadata>\n",
      "                <source>\n",
      "                    data/RAGwithLangChain.pdf\n",
      "                </source>\n",
      "                <page>\n",
      "                    5\n",
      "                </page>\n",
      "            </metadata>\n",
      "        </document>\n",
      "\n",
      "<document>\n",
      "            <content>\n",
      "                Introduction to LangChain\n",
      "Chain Vectorstore\n",
      "Document\n",
      "Memory\n",
      "loader\n",
      "🦜🔗\n",
      "Embedding\n",
      "Tool\n",
      "model\n",
      "Graph LM\n",
      "Agent\n",
      "            </content>\n",
      "            <metadata>\n",
      "                <source>\n",
      "                    data/RAGwithLangChain.pdf\n",
      "                </source>\n",
      "                <page>\n",
      "                    6\n",
      "                </page>\n",
      "            </metadata>\n",
      "        </document>\n",
      "\n",
      "<document>\n",
      "            <content>\n",
      "                Comparative analysis of RAG Frameworks\n",
      "Chain\n",
      "Vectorstore\n",
      "Memory\n",
      "Document\n",
      "loader\n",
      "🦙\n",
      "🦜🔗\n",
      "Tool\n",
      "Embedding\n",
      "model\n",
      "Graph\n",
      "LM\n",
      "Agent\n",
      "            </content>\n",
      "            <metadata>\n",
      "                <source>\n",
      "                    data/RAGwithLangChain.pdf\n",
      "                </source>\n",
      "                <page>\n",
      "                    8\n",
      "                </page>\n",
      "            </metadata>\n",
      "        </document>\n",
      "\n",
      "<document>\n",
      "            <content>\n",
      "                Recap\n",
      "LangGraph\n",
      "            </content>\n",
      "            <metadata>\n",
      "                <source>\n",
      "                    data/RAGwithLangChain.pdf\n",
      "                </source>\n",
      "                <page>\n",
      "                    13\n",
      "                </page>\n",
      "            </metadata>\n",
      "        </document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mgenerate\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "RAG는 Retrieval-Augmented Generation의 약자로, 정보 검색과 생성 모델을 결합하여 사용자 쿼리에 대한 답변을 생성하는 시스템입니다.\n",
      "\n",
      "**Source**\n",
      "- data/RAGwithLangChain.pdf (page 1)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langgraph.errors import GraphRecursionError\n",
    "\n",
    "config = RunnableConfig(recursion_limit=10, configurable={\"thread_id\": \"1\"})\n",
    "\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        (\"user\", \"RAG와 Langchain의 차이\"),\n",
    "    ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    invoke_graph(app, inputs, config)\n",
    "except GraphRecursionError as recursion_error:\n",
    "    print(f\"GraphRecursionError: {recursion_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2a8d8b9a-e43d-45ed-be0e-b41f5231e351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36magent\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mretrieve\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{\"binary_score\":\"no\"}=== [DECISION: DOCS IRRELEVENT] ===\n",
      "<document>\n",
      "            <content>\n",
      "                생성하시오\n",
      "다\"\n",
      "chunk1:\n",
      "chunk2: \"2024년 1월부터 최\n",
      "chunk2:\n",
      "저시급이 9860원으로 인상되었\n",
      "다\"\n",
      "user question:\n",
      "            </content>\n",
      "            <metadata>\n",
      "                <source>\n",
      "                    data/RAGwithLangChain.pdf\n",
      "                </source>\n",
      "                <page>\n",
      "                    3\n",
      "                </page>\n",
      "            </metadata>\n",
      "        </document>\n",
      "\n",
      "<document>\n",
      "            <content>\n",
      "                🦜\n",
      "생성하시오\n",
      "다\"\n",
      "chunk1:\n",
      "chunk2: \"2024년 1월부터 최\n",
      "chunk2:\n",
      "저시급이 9860원으로 인상되었\n",
      "다\"\n",
      "user question:\n",
      "            </content>\n",
      "            <metadata>\n",
      "                <source>\n",
      "                    data/RAGwithLangChain.pdf\n",
      "                </source>\n",
      "                <page>\n",
      "                    5\n",
      "                </page>\n",
      "            </metadata>\n",
      "        </document>\n",
      "\n",
      "<document>\n",
      "            <content>\n",
      "                🦙\n",
      "생성하시오\n",
      "다\"\n",
      "chunk1:\n",
      "chunk2: \"2024년 1월부터 최\n",
      "chunk2:\n",
      "저시급이 9860원으로 인상되었\n",
      "다\"\n",
      "user question:\n",
      "            </content>\n",
      "            <metadata>\n",
      "                <source>\n",
      "                    data/RAGwithLangChain.pdf\n",
      "                </source>\n",
      "                <page>\n",
      "                    4\n",
      "                </page>\n",
      "            </metadata>\n",
      "        </document>\n",
      "\n",
      "<document>\n",
      "            <content>\n",
      "                Recap\n",
      "Ollama\n",
      "            </content>\n",
      "            <metadata>\n",
      "                <source>\n",
      "                    data/RAGwithLangChain.pdf\n",
      "                </source>\n",
      "                <page>\n",
      "                    12\n",
      "                </page>\n",
      "            </metadata>\n",
      "        </document>\n",
      "\n",
      "<document>\n",
      "            <content>\n",
      "                CLIP\n",
      "Recap\n",
      "            </content>\n",
      "            <metadata>\n",
      "                <source>\n",
      "                    data/RAGwithLangChain.pdf\n",
      "                </source>\n",
      "                <page>\n",
      "                    9\n",
      "                </page>\n",
      "            </metadata>\n",
      "        </document>\n",
      "\n",
      "<document>\n",
      "            <content>\n",
      "                시급이며 교통비가 별도 지급된\n",
      "🦜🔗\n",
      "생성하시오\n",
      "다\"\n",
      "chunk1:\n",
      "chunk2: \"2024년 1월부터 최\n",
      "chunk2:\n",
      "저시급이 9860원으로 인상되었\n",
      "다\"\n",
      "top_k=10 user question:\n",
      "mode=compact\n",
      "            </content>\n",
      "            <metadata>\n",
      "                <source>\n",
      "                    data/RAGwithLangChain.pdf\n",
      "                </source>\n",
      "                <page>\n",
      "                    10\n",
      "                </page>\n",
      "            </metadata>\n",
      "        </document>\n",
      "\n",
      "<document>\n",
      "            <content>\n",
      "                Recap\n",
      "RAG Pipeline\n",
      "User query\n",
      "2024년 최저시급은 얼마야?\n",
      "2024년 한국의 최저임금은 9860원입니다.\n",
      "prompt=\"given\n",
      "embed_model=bge\n",
      "chunk_size=128 context...\"\n",
      "llm_model=mistral\n",
      "Indexing Retrieval Generation\n",
      "chunks Relevant 다음 주어지는 문서를 참고해\n",
      "documents chunk1: \"임금은 2024년 최저\n",
      "embedding documents user question에 대한 답변을\n",
      "시급이며 교통비가 별도 지급된\n",
      "            </content>\n",
      "            <metadata>\n",
      "                <source>\n",
      "                    data/RAGwithLangChain.pdf\n",
      "                </source>\n",
      "                <page>\n",
      "                    10\n",
      "                </page>\n",
      "            </metadata>\n",
      "        </document>\n",
      "\n",
      "<document>\n",
      "            <content>\n",
      "                Introduction to LangChain\n",
      "RAG Pipeline\n",
      "User query\n",
      "2024년 최저시급은 얼마야?\n",
      "2024년 한국의 최저임금은 9860원입니다.\n",
      "Indexing Retrieval Generation\n",
      "chunks Relevant 다음 주어지는 문서를 참고해\n",
      "documents chunk1: \"임금은 2024년 최저\n",
      "embedding documents user question에 대한 답변을\n",
      "Lla시급m이며 교통a비가 별I도n 지급된dex\n",
      "🦙\n",
      "생성하시오\n",
      "다\"\n",
      "chunk1:\n",
      "chunk2: \"2024년 1월부터 최\n",
      "            </content>\n",
      "            <metadata>\n",
      "                <source>\n",
      "                    data/RAGwithLangChain.pdf\n",
      "                </source>\n",
      "                <page>\n",
      "                    4\n",
      "                </page>\n",
      "            </metadata>\n",
      "        </document>\n",
      "\n",
      "<document>\n",
      "            <content>\n",
      "                Recap\n",
      "Chain\n",
      "Vectorstore\n",
      "Memory\n",
      "Document\n",
      "loader\n",
      "🦙\n",
      "🦜🔗\n",
      "Tool\n",
      "Embedding\n",
      "model\n",
      "Graph\n",
      "LM\n",
      "Agent\n",
      "            </content>\n",
      "            <metadata>\n",
      "                <source>\n",
      "                    data/RAGwithLangChain.pdf\n",
      "                </source>\n",
      "                <page>\n",
      "                    11\n",
      "                </page>\n",
      "            </metadata>\n",
      "        </document>\n",
      "\n",
      "<document>\n",
      "            <content>\n",
      "                CLIP\n",
      "Comparative Analysis of RAG\n",
      "Frameworks\n",
      "            </content>\n",
      "            <metadata>\n",
      "                <source>\n",
      "                    data/RAGwithLangChain.pdf\n",
      "                </source>\n",
      "                <page>\n",
      "                    7\n",
      "                </page>\n",
      "            </metadata>\n",
      "        </document>=== [REWRITE QUERY] ===\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mrewrite\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "To improve the question while maintaining its original intent, you could ask:\n",
      "\n",
      "\"이영진은 어떤 직업을 가진 사람인가요?\" \n",
      "\n",
      "This translates to \"What is the profession of Lee Young-jin?\" which provides a clearer focus on the person's occupation or role.\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36magent\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "이영진에 대한 정보는 제공되지 않았습니다. 이영진이 어떤 사람인지에 대한 구체적인 정보가 필요하다면, 그가 어떤 분야에서 활동하는지, 또는 어떤 업적이 있는지에 대한 추가적인 맥락을 제공해 주시면 더 도움이 될 수 있습니다."
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'__end__'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m inputs = {\n\u001b[32m      2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m      3\u001b[39m         (\u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m이영진은 뭐하는사람이야?\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      4\u001b[39m     ]\n\u001b[32m      5\u001b[39m }\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[43mstream_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43magent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mretrieve\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrewrite\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgenerate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m GraphRecursionError \u001b[38;5;28;01mas\u001b[39;00m recursion_error:\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGraphRecursionError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecursion_error\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mstream_graph\u001b[39m\u001b[34m(graph, inputs, config, node_names, callback)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[33;03mLangGraph의 실행 결과를 스트리밍하여 출력하는 함수입니다.\u001b[39;00m\n\u001b[32m     13\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m \u001b[33;03m    None: 함수는 스트리밍 결과를 출력만 하고 반환값은 없습니다.\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     25\u001b[39m prev_node = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk_msg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcurr_node\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlanggraph_node\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# node_names가 비어있거나 현재 노드가 node_names에 있는 경우에만 처리\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Langgraph-handson\\lg-handson-env\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2461\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2455\u001b[39m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[32m   2456\u001b[39m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[32m   2457\u001b[39m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[32m   2458\u001b[39m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[32m   2459\u001b[39m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[32m   2460\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m loop.tick(input_keys=\u001b[38;5;28mself\u001b[39m.input_channels):\n\u001b[32m-> \u001b[39m\u001b[32m2461\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2462\u001b[39m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2463\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2464\u001b[39m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2465\u001b[39m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2466\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2467\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2468\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2469\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Langgraph-handson\\lg-handson-env\\Lib\\site-packages\\langgraph\\pregel\\runner.py:247\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[39m\n\u001b[32m    245\u001b[39m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m.\u001b[49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpanic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    252\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tb := exc.__traceback__:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Langgraph-handson\\lg-handson-env\\Lib\\site-packages\\langgraph\\pregel\\runner.py:499\u001b[39m, in \u001b[36m_panic_or_proceed\u001b[39m\u001b[34m(futs, timeout_exc_cls, panic)\u001b[39m\n\u001b[32m    497\u001b[39m                 interrupts.append(exc)\n\u001b[32m    498\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m499\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    500\u001b[39m \u001b[38;5;66;03m# raise combined interrupts\u001b[39;00m\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m interrupts:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Langgraph-handson\\lg-handson-env\\Lib\\site-packages\\langgraph\\pregel\\executor.py:80\u001b[39m, in \u001b[36mBackgroundExecutor.done\u001b[39m\u001b[34m(self, task)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Remove the task from the tasks dict when it's done.\"\"\"\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m GraphBubbleUp:\n\u001b[32m     82\u001b[39m     \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[32m     83\u001b[39m     \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[32m     84\u001b[39m     \u001b[38;5;28mself\u001b[39m.tasks.pop(task)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.1008.0_x64__qbz5n2kfra8p0\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.1008.0_x64__qbz5n2kfra8p0\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.1008.0_x64__qbz5n2kfra8p0\\Lib\\concurrent\\futures\\thread.py:59\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Langgraph-handson\\lg-handson-env\\Lib\\site-packages\\langgraph\\pregel\\retry.py:40\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     38\u001b[39m     task.writes.clear()\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     42\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Langgraph-handson\\lg-handson-env\\Lib\\site-packages\\langgraph\\utils\\runnable.py:625\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    623\u001b[39m                 \u001b[38;5;28minput\u001b[39m = context.run(step.invoke, \u001b[38;5;28minput\u001b[39m, config, **kwargs)\n\u001b[32m    624\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m             \u001b[38;5;28minput\u001b[39m = \u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Langgraph-handson\\lg-handson-env\\Lib\\site-packages\\langgraph\\utils\\runnable.py:377\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    375\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Langgraph-handson\\lg-handson-env\\Lib\\site-packages\\langgraph\\graph\\branch.py:174\u001b[39m, in \u001b[36mBranch._route\u001b[39m\u001b[34m(self, input, config, reader, writer)\u001b[39m\n\u001b[32m    172\u001b[39m     value = \u001b[38;5;28minput\u001b[39m\n\u001b[32m    173\u001b[39m result = \u001b[38;5;28mself\u001b[39m.path.invoke(value, config)\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Langgraph-handson\\lg-handson-env\\Lib\\site-packages\\langgraph\\graph\\branch.py:210\u001b[39m, in \u001b[36mBranch._finish\u001b[39m\u001b[34m(self, writer, input, result, config)\u001b[39m\n\u001b[32m    207\u001b[39m     result = [result]\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ends:\n\u001b[32m    209\u001b[39m     destinations: Sequence[Union[Send, \u001b[38;5;28mstr\u001b[39m]] = [\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m         r \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(r, Send) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mends\u001b[49m\u001b[43m[\u001b[49m\u001b[43mr\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m result\n\u001b[32m    211\u001b[39m     ]\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    213\u001b[39m     destinations = cast(Sequence[Union[Send, \u001b[38;5;28mstr\u001b[39m]], result)\n",
      "\u001b[31mKeyError\u001b[39m: '__end__'",
      "During task with name 'agent' and id 'f08f566a-e438-6cba-4831-7de0a0e783b5'"
     ]
    }
   ],
   "source": [
    "inputs = {\n",
    "    \"messages\": [\n",
    "        (\"user\", \"이영진은 뭐하는사람이야?\"),\n",
    "    ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    stream_graph(app, inputs, config, [\"agent\", \"retrieve\", \"rewrite\", \"generate\"])\n",
    "    \n",
    "except GraphRecursionError as recursion_error:\n",
    "    print(f\"GraphRecursionError: {recursion_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cdb598b4-c743-4aa0-8985-b597763acc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: {'agent': {'messages': [AIMessage(content='RAG는 Retrieval-Augmented Generation의 약자로, 정보 검색과 생성 모델을 결합하여 사용자 쿼리에 대한 답변을 생성하는 시스템입니다. 이 시스템은 사용자의 질문에 대해 관련 정보를 검색하고, 이를 바탕으로 자연어로 응답을 생성합니다. \\n\\n**출처**\\n- data/RAGwithLangChain.pdf (페이지 10)', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0392822090', 'service_tier': 'default'}, id='run--1d6647b5-10c1-46e8-825c-8a7ac205a39f-0')]}}\n",
      "Node: agent\n"
     ]
    }
   ],
   "source": [
    "for step in app.stream(user_input, config=config):\n",
    "    print(\"step:\", step)\n",
    "    for node_name, output in step.items():\n",
    "        print(\"Node:\", node_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce7a0c1-31de-4dcd-ba53-c8e876d709b5",
   "metadata": {},
   "source": [
    "### Where RAG is unnecessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d763994-b35f-4452-9161-7c4d99401130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36magent\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "대한민국의 수도는 서울입니다."
     ]
    }
   ],
   "source": [
    "# 문서 검색이 불가능한 질문 예시\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        (\"user\", \"대한민국의 수도는?\"),\n",
    "    ]\n",
    "}\n",
    "try:\n",
    "    stream_graph(app, inputs, config, [\"agent\", \"rewrite\", \"generate\"])\n",
    "except GraphRecursionError as recursion_error:\n",
    "    print(f\"GraphRecursionError: {recursion_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0066e889-412c-4a94-b6f0-d1b2746dd5d5",
   "metadata": {},
   "source": [
    "### Where it is impossible to get info from the docs using RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3cfe02cf-adb5-4ad7-a682-f0157f3bfcb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36magent\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "죄송하지만, 현재로서는 귀하의 이름을 알 수 있는 정보가 없습니다. 이름을 알려주시면 그에 맞춰 대화할 수 있습니다!"
     ]
    }
   ],
   "source": [
    "# 문서 검색이 불가능한 질문 예시\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        (\"user\", \"내 이름을 알려줘\"),\n",
    "    ]\n",
    "}\n",
    "try:\n",
    "    stream_graph(app, inputs, config, [\"agent\", \"rewrite\", \"generate\"])\n",
    "except GraphRecursionError as recursion_error:\n",
    "    print(f\"GraphRecursionError: {recursion_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4b7c6a-6f73-4f84-abf9-5ee3f2c9143a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
