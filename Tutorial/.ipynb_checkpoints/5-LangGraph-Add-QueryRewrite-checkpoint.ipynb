{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "699129b7-687e-4b5a-82e1-75ce5ffb4742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de4b407f-06f1-487c-8925-65bb7b9d0d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/RAGwithLangChain.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    }
   ],
   "source": [
    "from rag.pdf import PDFRetrievalChain\n",
    "\n",
    "# PDF 문서를 로드합니다.\n",
    "pdf = PDFRetrievalChain([\"data/RAGwithLangChain.pdf\"]).create_chain()\n",
    "\n",
    "# retriever와 chain을 생성합니다.\n",
    "pdf_retriever = pdf.retriever\n",
    "pdf_chain = pdf.chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e71c17e-af01-4e67-88a5-7851d0840db2",
   "metadata": {},
   "source": [
    "### Message tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2312f96-1ceb-4d0e-9637-6cf6e3d7aacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_role_from_messages(msg):\n",
    "    if isinstance(msg, HumanMessage):\n",
    "        return \"user\"\n",
    "    elif isinstance(msg, AIMessage):\n",
    "        return \"assistant\"\n",
    "    else:\n",
    "        return \"assistant\"\n",
    "\n",
    "def messages_to_history(messages):\n",
    "    result =  \"\\n\".join(\n",
    "        [f\"{get_role_from_messages(msg)}: {msg.content}\" for msg in messages]\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4f004e-da07-489e-ac09-7c3b299563f5",
   "metadata": {},
   "source": [
    "### GroundednessCheck tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "916bf63b-bb3e-4bbf-90d8-ab73e6c8fa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from rag.utils import format_docs\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from typing import Any, Dict, List, Callable, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "class GroundnessQuestionScore(BaseModel):\n",
    "    \"\"\"Binary scores for relevance checks\"\"\"\n",
    "\n",
    "    score: str = Field(\n",
    "        description=\"relevant or not relevant. Answer 'yes' if the answer is relevant to the question else answer 'no'\"\n",
    "    )\n",
    "\n",
    "class GroundnessAnswerRetrievalScore(BaseModel):\n",
    "    \"\"\"Binary scores for relevance checks\"\"\"\n",
    "\n",
    "    score: str = Field(\n",
    "        description=\"relevant or not relevant. Answer 'yes' if the answer is relevant to the retrieved document else answer 'no'\"\n",
    "    )\n",
    "\n",
    "class GroundnessQuestionRetrievalScore(BaseModel):\n",
    "    \"\"\"Binary scores for relevance checks\"\"\"\n",
    "\n",
    "    score: str = Field(\n",
    "        description=\"relevant or not relevant. Answer 'yes' if the question is relevant to the retrieved document else answer 'no'\"\n",
    "    )\n",
    "\n",
    "class GroundednessChecker:\n",
    "    \"\"\"\n",
    "    GroundednessChecker 클래스는 문서의 정확성을 평가하는 클래스입니다.\n",
    "\n",
    "    이 클래스는 주어진 문서가 정확한지 여부를 평가합니다.\n",
    "    'yes' 또는 'no' 두 가지 중 하나를 반환합니다.\n",
    "\n",
    "    Attributes:\n",
    "        llm (BaseLLM): 사용할 언어 모델 인스턴스\n",
    "        target (str): 평가 대상 ('retrieval-answer', 'question-answer' 또는 'question-retrieval')\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, llm, target=\"retrieval-answer\"):\n",
    "        \"\"\"\n",
    "        GroundednessChecker 클래스의 생성자입니다.\n",
    "\n",
    "        Args:\n",
    "            llm (BaseLLM): 사용할 언어 모델 인스턴스\n",
    "            target (str): 평가 대상 ('retrieval-answer', 'question-answer' 또는 'question-retrieval')\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.target = target\n",
    "\n",
    "    def create(self):\n",
    "        \"\"\"\n",
    "        정확성 평가를 위한 체인을 생성합니다.\n",
    "\n",
    "        Returns:\n",
    "            Chain: 정확성 평가를 수행할 수 있는 체인 객체\n",
    "        \"\"\"\n",
    "        # 파서\n",
    "        if self.target == \"retrieval-answer\":\n",
    "            llm = self.llm.with_structured_output(GroundnessAnswerRetrievalScore)\n",
    "        elif self.target == \"question-answer\":\n",
    "            llm = self.llm.with_structured_output(GroundnessQuestionScore)\n",
    "        elif self.target == \"question-retrieval\":\n",
    "            llm = self.llm.with_structured_output(GroundnessQuestionRetrievalScore)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid target: {self.target}\")\n",
    "\n",
    "        # 프롬프트 선택\n",
    "        if self.target == \"retrieval-answer\":\n",
    "            template = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "                Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "                Here is the answer: {answer} \\n\n",
    "                If the document contains keyword(s) or semantic meaning related to the user answer, grade it as relevant. \\n\n",
    "                \n",
    "                Give a binary score 'yes' or 'no' score to indicate whether the retrieved document is relevant to the answer.\"\"\"\n",
    "            input_vars = [\"context\", \"answer\"]\n",
    "\n",
    "        elif self.target == \"question-answer\":\n",
    "            template = \"\"\"You are a grader assessing whether an answer appropriately addresses the given question. \\n\n",
    "                Here is the question: \\n\\n {question} \\n\\n\n",
    "                Here is the answer: {answer} \\n\n",
    "                If the answer directly addresses the question and provides relevant information, grade it as relevant. \\n\n",
    "                Consider both semantic meaning and factual accuracy in your assessment. \\n\n",
    "                \n",
    "                Give a binary score 'yes' or 'no' score to indicate whether the answer is relevant to the question.\"\"\"\n",
    "            input_vars = [\"question\", \"answer\"]\n",
    "\n",
    "        elif self.target == \"question-retrieval\":\n",
    "            template = \"\"\"You are a grader assessing whether a retrieved document is relevant to the given question. \\n\n",
    "                Here is the question: \\n\\n {question} \\n\\n\n",
    "                Here is the retrieved document: \\n\\n {context} \\n\n",
    "                If the document contains information that could help answer the question, grade it as relevant. \\n\n",
    "                Consider both semantic meaning and potential usefulness for answering the question. \\n\n",
    "                \n",
    "                Give a binary score 'yes' or 'no' score to indicate whether the retrieved document is relevant to the question.\"\"\"\n",
    "            input_vars = [\"question\", \"context\"]\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid target: {self.target}\")\n",
    "\n",
    "        # 프롬프트 생성\n",
    "        prompt = PromptTemplate(\n",
    "            template=template,\n",
    "            input_variables=input_vars,\n",
    "        )\n",
    "\n",
    "        # 체인\n",
    "        chain = prompt | llm\n",
    "        return chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9196cd-8c60-4264-ba3d-752fe2b1f410",
   "metadata": {},
   "source": [
    "### Invoking / Streaming tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a83ca7b5-cf84-4e07-a370-c372b37e12d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def stream_graph(\n",
    "    graph: CompiledStateGraph,\n",
    "    inputs: dict,\n",
    "    config: RunnableConfig,\n",
    "    node_names: List[str] = [],\n",
    "    callback: Callable = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    LangGraph의 실행 결과를 스트리밍하여 출력하는 함수입니다.\n",
    "\n",
    "    Args:\n",
    "        graph (CompiledStateGraph): 실행할 컴파일된 LangGraph 객체\n",
    "        inputs (dict): 그래프에 전달할 입력값 딕셔너리\n",
    "        config (RunnableConfig): 실행 설정\n",
    "        node_names (List[str], optional): 출력할 노드 이름 목록. 기본값은 빈 리스트\n",
    "        callback (Callable, optional): 각 청크 처리를 위한 콜백 함수. 기본값은 None\n",
    "            콜백 함수는 {\"node\": str, \"content\": str} 형태의 딕셔너리를 인자로 받습니다.\n",
    "\n",
    "    Returns:\n",
    "        None: 함수는 스트리밍 결과를 출력만 하고 반환값은 없습니다.\n",
    "    \"\"\"\n",
    "    prev_node = \"\"\n",
    "    for chunk_msg, metadata in graph.stream(inputs, config, stream_mode=\"messages\"):\n",
    "        curr_node = metadata[\"langgraph_node\"]\n",
    "\n",
    "        # node_names가 비어있거나 현재 노드가 node_names에 있는 경우에만 처리\n",
    "        if not node_names or curr_node in node_names:\n",
    "            # 콜백 함수가 있는 경우 실행\n",
    "            if callback:\n",
    "                callback({\"node\": curr_node, \"content\": chunk_msg.content})\n",
    "            # 콜백이 없는 경우 기본 출력\n",
    "            else:\n",
    "                # 노드가 변경된 경우에만 구분선 출력\n",
    "                if curr_node != prev_node:\n",
    "                    print(\"\\n\" + \"=\" * 50)\n",
    "                    print(f\"🔄 Node: \\033[1;36m{curr_node}\\033[0m 🔄\")\n",
    "                    print(\"- \" * 25)\n",
    "                print(chunk_msg.content, end=\"\", flush=True)\n",
    "\n",
    "            prev_node = curr_node\n",
    "\n",
    "\n",
    "def invoke_graph(\n",
    "    graph: CompiledStateGraph,\n",
    "    inputs: dict,\n",
    "    config: RunnableConfig,\n",
    "    node_names: List[str] = [],\n",
    "    callback: Callable = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    LangGraph 앱의 실행 결과를 예쁘게 스트리밍하여 출력하는 함수입니다.\n",
    "\n",
    "    Args:\n",
    "        graph (CompiledStateGraph): 실행할 컴파일된 LangGraph 객체\n",
    "        inputs (dict): 그래프에 전달할 입력값 딕셔너리\n",
    "        config (RunnableConfig): 실행 설정\n",
    "        node_names (List[str], optional): 출력할 노드 이름 목록. 기본값은 빈 리스트\n",
    "        callback (Callable, optional): 각 청크 처리를 위한 콜백 함수. 기본값은 None\n",
    "            콜백 함수는 {\"node\": str, \"content\": str} 형태의 딕셔너리를 인자로 받습니다.\n",
    "\n",
    "    Returns:\n",
    "        None: 함수는 스트리밍 결과를 출력만 하고 반환값은 없습니다.\n",
    "    \"\"\"\n",
    "\n",
    "    def format_namespace(namespace):\n",
    "        return namespace[-1].split(\":\")[0] if len(namespace) > 0 else \"root graph\"\n",
    "\n",
    "    # subgraphs=True 를 통해 서브그래프의 출력도 포함\n",
    "    for namespace, chunk in graph.stream(\n",
    "        inputs, config, stream_mode=\"updates\", subgraphs=True\n",
    "    ):\n",
    "        for node_name, node_chunk in chunk.items():\n",
    "            # node_names가 비어있지 않은 경우에만 필터링\n",
    "            if len(node_names) > 0 and node_name not in node_names:\n",
    "                continue\n",
    "\n",
    "            # 콜백 함수가 있는 경우 실행\n",
    "            if callback is not None:\n",
    "                callback({\"node\": node_name, \"content\": node_chunk})\n",
    "            # 콜백이 없는 경우 기본 출력\n",
    "            else:\n",
    "                print(\"\\n\" + \"=\" * 50)\n",
    "                formatted_namespace = format_namespace(namespace)\n",
    "                if formatted_namespace == \"root graph\":\n",
    "                    print(f\"🔄 Node: \\033[1;36m{node_name}\\033[0m 🔄\")\n",
    "                else:\n",
    "                    print(\n",
    "                        f\"🔄 Node: \\033[1;36m{node_name}\\033[0m in [\\033[1;33m{formatted_namespace}\\033[0m] 🔄\"\n",
    "                    )\n",
    "                print(\"- \" * 25)\n",
    "\n",
    "                # 노드의 청크 데이터 출력\n",
    "                if isinstance(node_chunk, dict):\n",
    "                    for k, v in node_chunk.items():\n",
    "                        if isinstance(v, BaseMessage):\n",
    "                            v.pretty_print()\n",
    "                        elif isinstance(v, list):\n",
    "                            for list_item in v:\n",
    "                                if isinstance(list_item, BaseMessage):\n",
    "                                    list_item.pretty_print()\n",
    "                                else:\n",
    "                                    print(list_item)\n",
    "                        elif isinstance(v, dict):\n",
    "                            for node_chunk_key, node_chunk_value in node_chunk.items():\n",
    "                                print(f\"{node_chunk_key}:\\n{node_chunk_value}\")\n",
    "                        else:\n",
    "                            print(f\"\\033[1;32m{k}\\033[0m:\\n{v}\")\n",
    "                else:\n",
    "                    if node_chunk is not None:\n",
    "                        for item in node_chunk:\n",
    "                            print(item)\n",
    "                print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d66fd4-6b71-43bb-9fd1-3c18ec6fa9c8",
   "metadata": {},
   "source": [
    "### Search tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "944a2270-64f1-4da4-9d61-ffa094d26d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import BaseTool\n",
    "from pydantic import BaseModel, Field\n",
    "from tavily import TavilyClient\n",
    "from typing import Literal, Sequence, Optional, List\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "class TavilySearchInput(BaseModel):\n",
    "    \"\"\"Input for the Tavily tool.\"\"\"\n",
    "\n",
    "    query: str = Field(description=\"검색 쿼리\")\n",
    "\n",
    "\n",
    "def format_search_result(result: dict, include_raw_content: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Utility functions for formatting search results.\n",
    "\n",
    "    Args:\n",
    "        result (dict): 원본 검색 결과\n",
    "\n",
    "    Returns:\n",
    "        str: XML 형식으로 포맷팅된 검색 결과\n",
    "    \"\"\"\n",
    "    # 한글 인코딩 처리를 위해 json.dumps() 사용\n",
    "    title = json.dumps(result[\"title\"], ensure_ascii=False)[1:-1]\n",
    "    content = json.dumps(result[\"content\"], ensure_ascii=False)[1:-1]\n",
    "    raw_content = \"\"\n",
    "    if (\n",
    "        include_raw_content\n",
    "        and \"raw_content\" in result\n",
    "        and result[\"raw_content\"] is not None\n",
    "        and len(result[\"raw_content\"].strip()) > 0\n",
    "    ):\n",
    "        raw_content = f\"<raw>{result['raw_content']}</raw>\"\n",
    "\n",
    "    return f\"<document><title>{title}</title><url>{result['url']}</url><content>{content}</content>{raw_content}</document>\"\n",
    "\n",
    "\n",
    "class TavilySearch(BaseTool):\n",
    "    \"\"\"\n",
    "    Tool that queries the Tavily Search API and gets back json\n",
    "    \"\"\"\n",
    "\n",
    "    name: str = \"tavily_web_search\"\n",
    "    description: str = (\n",
    "        \"A search engine optimized for comprehensive, accurate, and trusted results. \"\n",
    "        \"Useful for when you need to answer questions about current events. \"\n",
    "        \"Input should be a search query. [IMPORTANT] Input(query) should be over 5 characters.\"\n",
    "    )\n",
    "    args_schema: type[BaseModel] = TavilySearchInput\n",
    "    client: TavilyClient = None\n",
    "    include_domains: list = []\n",
    "    exclude_domains: list = []\n",
    "    max_results: int = 3\n",
    "    topic: Literal[\"general\", \"news\"] = \"general\"\n",
    "    days: int = 3\n",
    "    search_depth: Literal[\"basic\", \"advanced\"] = \"basic\"\n",
    "    include_answer: bool = False\n",
    "    include_raw_content: bool = True\n",
    "    include_images: bool = False\n",
    "    format_output: bool = False\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key: Optional[str] = None,\n",
    "        include_domains: list = [],\n",
    "        exclude_domains: list = [],\n",
    "        max_results: int = 3,\n",
    "        topic: Literal[\"general\", \"news\"] = \"general\",\n",
    "        days: int = 3,\n",
    "        search_depth: Literal[\"basic\", \"advanced\"] = \"basic\",\n",
    "        include_answer: bool = False,\n",
    "        include_raw_content: bool = True,\n",
    "        include_images: bool = False,\n",
    "        format_output: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        TavilySearch 클래스의 인스턴스를 초기화합니다.\n",
    "\n",
    "        Args:\n",
    "            api_key (str): Tavily API 키\n",
    "            include_domains (list): 검색에 포함할 도메인 목록\n",
    "            exclude_domains (list): 검색에서 제외할 도메인 목록\n",
    "            max_results (int): 기본 검색 결과 수\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if api_key is None:\n",
    "            api_key = os.environ.get(\"TAVILY_API_KEY\", None)\n",
    "\n",
    "        if api_key is None:\n",
    "            raise ValueError(\"Tavily API key is not set.\")\n",
    "\n",
    "        self.client = TavilyClient(api_key=api_key)\n",
    "        self.include_domains = include_domains\n",
    "        self.exclude_domains = exclude_domains\n",
    "        self.max_results = max_results\n",
    "        self.topic = topic\n",
    "        self.days = days\n",
    "        self.search_depth = search_depth\n",
    "        self.include_answer = include_answer\n",
    "        self.include_raw_content = include_raw_content\n",
    "        self.include_images = include_images\n",
    "        self.format_output = format_output\n",
    "\n",
    "    def _run(self, query: str) -> str:\n",
    "        \"\"\"BaseTool의 _run 메서드 구현\"\"\"\n",
    "        results = self.search(query)\n",
    "        return results\n",
    "        # return json.dumps(results, ensure_ascii=False)\n",
    "\n",
    "    def search(\n",
    "        self,\n",
    "        query: str,\n",
    "        search_depth: Literal[\"basic\", \"advanced\"] = None,\n",
    "        topic: Literal[\"general\", \"news\"] = None,\n",
    "        days: int = None,\n",
    "        max_results: int = None,\n",
    "        include_domains: Sequence[str] = None,\n",
    "        exclude_domains: Sequence[str] = None,\n",
    "        include_answer: bool = None,\n",
    "        include_raw_content: bool = None,\n",
    "        include_images: bool = None,\n",
    "        format_output: bool = None,\n",
    "        **kwargs,\n",
    "    ) -> list:\n",
    "        \"\"\"\n",
    "        검색을 수행하고 결과를 반환합니다.\n",
    "\n",
    "        Args:\n",
    "            query (str): 검색 쿼리\n",
    "            search_depth (str): 검색 깊이 (\"basic\" 또는 \"advanced\")\n",
    "            topic (str): 검색 주제 (\"general\" 또는 \"news\")\n",
    "            days (int): 검색할 날짜 범위\n",
    "            max_results (int): 최대 검색 결과 수\n",
    "            include_domains (list): 검색에 포함할 도메인 목록\n",
    "            exclude_domains (list): 검색에서 제외할 도메인 목록\n",
    "            include_answer (bool): 답변 포함 여부\n",
    "            include_raw_content (bool): 원본 콘텐츠 포함 여부\n",
    "            include_images (bool): 이미지 포함 여부\n",
    "            format_output (bool): 결과를 포맷팅할지 여부\n",
    "            **kwargs: 추가 키워드 인자\n",
    "\n",
    "        Returns:\n",
    "            list: 검색 결과 목록\n",
    "        \"\"\"\n",
    "        # 기본값 설정\n",
    "        params = {\n",
    "            \"query\": query,\n",
    "            \"search_depth\": search_depth or self.search_depth,\n",
    "            \"topic\": topic or self.topic,\n",
    "            \"max_results\": max_results or self.max_results,\n",
    "            \"include_domains\": include_domains or self.include_domains,\n",
    "            \"exclude_domains\": exclude_domains or self.exclude_domains,\n",
    "            \"include_answer\": (\n",
    "                include_answer if include_answer is not None else self.include_answer\n",
    "            ),\n",
    "            \"include_raw_content\": (\n",
    "                include_raw_content\n",
    "                if include_raw_content is not None\n",
    "                else self.include_raw_content\n",
    "            ),\n",
    "            \"include_images\": (\n",
    "                include_images if include_images is not None else self.include_images\n",
    "            ),\n",
    "            **kwargs,\n",
    "        }\n",
    "\n",
    "        # days 파라미터 처리\n",
    "        if days is not None:\n",
    "            if params[\"topic\"] == \"general\":\n",
    "                print(\n",
    "                    \"Warning: days parameter is ignored for 'general' topic search. Set topic parameter to 'news' to use days.\"\n",
    "                )\n",
    "            else:\n",
    "                params[\"days\"] = days\n",
    "\n",
    "        # API 호출\n",
    "        response = self.client.search(**params)\n",
    "\n",
    "        # 결과 포맷팅\n",
    "        format_output = (\n",
    "            format_output if format_output is not None else self.format_output\n",
    "        )\n",
    "        if format_output:\n",
    "            return [\n",
    "                format_search_result(r, params[\"include_raw_content\"])\n",
    "                for r in response[\"results\"]\n",
    "            ]\n",
    "        else:\n",
    "            return response[\"results\"]\n",
    "\n",
    "    def get_search_context(\n",
    "        self,\n",
    "        query: str,\n",
    "        search_depth: Literal[\"basic\", \"advanced\"] = \"basic\",\n",
    "        topic: Literal[\"general\", \"news\"] = \"general\",\n",
    "        days: int = 3,\n",
    "        max_results: int = 5,\n",
    "        include_domains: Sequence[str] = None,\n",
    "        exclude_domains: Sequence[str] = None,\n",
    "        max_tokens: int = 4000,\n",
    "        format_output: bool = True,\n",
    "        **kwargs,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        검색 쿼리에 대한 컨텍스트를 가져옵니다. 웹사이트에서 관련 콘텐츠만 가져오는 데 유용하며,\n",
    "        컨텍스트 추출과 제한을 직접 처리할 필요가 없습니다.\n",
    "\n",
    "        Args:\n",
    "            query (str): 검색 쿼리\n",
    "            search_depth (str): 검색 깊이 (\"basic\" 또는 \"advanced\")\n",
    "            topic (str): 검색 주제 (\"general\" 또는 \"news\")\n",
    "            days (int): 검색할 날짜 범위\n",
    "            max_results (int): 최대 검색 결과 수\n",
    "            include_domains (list): 검색에 포함할 도메인 목록\n",
    "            exclude_domains (list): 검색에서 제외할 도메인 목록\n",
    "            max_tokens (int): 반환할 최대 토큰 수 (openai 토큰 계산 기준). 기본값은 4000입니다.\n",
    "            format_output (bool): 결과를 포맷팅할지 여부\n",
    "            **kwargs: 추가 키워드 인자\n",
    "\n",
    "        Returns:\n",
    "            str: 컨텍스트 제한까지의 검색 컨텍스트를 포함하는 JSON 문자열\n",
    "        \"\"\"\n",
    "        response = self.client.search(\n",
    "            query,\n",
    "            search_depth=search_depth,\n",
    "            topic=topic,\n",
    "            days=days,\n",
    "            max_results=max_results,\n",
    "            include_domains=include_domains,\n",
    "            exclude_domains=exclude_domains,\n",
    "            include_answer=False,\n",
    "            include_raw_content=False,\n",
    "            include_images=False,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        sources = response.get(\"results\", [])\n",
    "        if format_output:\n",
    "            context = [\n",
    "                format_search_result(source, include_raw_content=False)\n",
    "                for source in sources\n",
    "            ]\n",
    "        else:\n",
    "            context = [\n",
    "                {\n",
    "                    \"url\": source[\"url\"],\n",
    "                    \"content\": json.dumps(\n",
    "                        {\"title\": source[\"title\"], \"content\": source[\"content\"]},\n",
    "                        ensure_ascii=False,\n",
    "                    ),\n",
    "                }\n",
    "                for source in sources\n",
    "            ]\n",
    "\n",
    "        # max_tokens 처리 로직은 여기에 구현해야 합니다.\n",
    "        # 현재는 간단히 모든 컨텍스트를 반환합니다.\n",
    "        return json.dumps(context, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c78fa4-49d9-483f-a349-83cfa8958037",
   "metadata": {},
   "source": [
    "## State Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9d92ad6-bc8b-4030-a748-76986b23b102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "from typing import Annotated\n",
    "\n",
    "class GraphState(MessagesState):\n",
    "    # messages: Annotated[list, add_messages] # pre-built in MessagesState\n",
    "    question: Annotated[str, \"Question\"]\n",
    "    context: Annotated[str, \"Context\"]\n",
    "    answer: Annotated[str, \"Answer\"]\n",
    "    relevance: Annotated[str, \"Relevance\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b40aa21-2f5e-40d0-940e-d064866bb6fd",
   "metadata": {},
   "source": [
    "## Node Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22ef4efd-36fe-4b04-8cef-7da3bef0d0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 검색 노드\n",
    "def retrieve_document(state: GraphState) -> GraphState:\n",
    "    latest_question = state[\"question\"]\n",
    "\n",
    "    retrieved_docs = pdf_retriever.invoke(latest_question)\n",
    "    retrieved_docs = format_docs(retrieved_docs)\n",
    "\n",
    "    return GraphState(context=retrieved_docs)\n",
    "\n",
    "# 답변 생성 노드\n",
    "def llm_answer(state: GraphState) -> GraphState:\n",
    "    latest_question = state[\"question\"]\n",
    "    context = state[\"context\"]\n",
    "\n",
    "    response = pdf_chain.invoke(\n",
    "        {\n",
    "            \"question\": latest_question,\n",
    "            \"context\": context,\n",
    "            \"chat_history\": messages_to_history(state[\"messages\"])\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"answer\": response,\n",
    "        \"messages\": [(\"user\", latest_question), (\"assistant\", response)],\n",
    "    }\n",
    "\n",
    "def relevance_check(state: GraphState) -> GraphState:\n",
    "    question_retrieval_relevance_checker = GroundednessChecker(\n",
    "        llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0), target=\"question-retrieval\"\n",
    "    ).create()\n",
    "\n",
    "    response = question_retrieval_relevance_checker.invoke(\n",
    "        {\"question\":state[\"question\"], \"context\":state[\"context\"]}\n",
    "    )\n",
    "\n",
    "    print(\"=== [RELEVENCE CHECKING ===\")\n",
    "\n",
    "    return {\"relevance\":response.score}\n",
    "\n",
    "def is_relevant(state: GraphState) -> GraphState:\n",
    "    if state[\"relevance\"] == \"yes\":\n",
    "        return \"relevant\"\n",
    "    else:\n",
    "        return \"not relevant\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a75bea6-23b0-4ac7-8b26-fc146fa99f37",
   "metadata": {},
   "source": [
    "## Web Search Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d1f9e20-024f-4fdf-a1f6-35beccfbcc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<document><title>[Ai 리뷰] 생성 Ai 모델, 정확성과 신뢰성 향상시키는 \\'검색 증강 생성(Rag)\\'은 무엇인가?</title><url>https://www.aitimes.kr/news/articleView.html?idxno=29431</url><content>\\'RAG\\' 단어를 만든 코히어(Cohere) 자연어 처리 연구원 패트릭 루이스(Patrick Lewis) 박사(사진:영상 갈무리) ... RAG를 언급했을 2020년에 패트릭 루이스는 유니버시티 칼리지 런던에서 NLP 박사 학위를 취득하고 런던의 새로운 AI 연구소에서 메타를 위해 일하고 있었다</content><raw>상단영역\\n\\n\\n\\n본문영역\\n\\n현재위치\\n\\n[AI 리뷰] 생성\\xa0AI 모델, 정확성과 신뢰성 향상시키는  \\'검색 증강 생성(RAG)\\'은 무엇인가?\\n\\nSNS 기사보내기\\n\\n패트릭 루이스는 RAG를 ‘범용 미세 조정 레시피’라고 칭했다. 이는 RAG가 거의 모든 LLM에서 모든 외부 리소스와 연결하는 데 사용...\\n\\n생성 AI의 최근 발전을 더 현실적으로 이해하기 위해 법정의 모습을 생각해 볼 수 있다.\\n\\n예를 들어, 판사는 법에 대한 일반적인 이해를 바탕으로 사건을 심리하고 판결한다. 의료 과실 소송이나 노동 분쟁과 같이 특별한 전문 지식이 필요한 사건의 경우, 판사는 법원 서기관에게 인용할 수 있는 판례와 구체적인 사례를 찾게 한다.\\n\\n훌륭한 판사처럼 대형언어모델(LLM)은 다양한 사람의 질의에 응답할 수 있다. 하지만 출처가 정확한 신뢰할 수 있는 답변을 제공하려면, 조사를 수행할 조수가 필요하다.\\n\\n바로 \\'검색 증강 생성(Retrieval Augmented Generation. 이하, RAG)\\'라고 하는 프로세스가 AI처럼 법원 서기관의 역할을 한다.\\n\\n현재, 기업용 인공지능에 중점을 두고 LLM을 전문으로 하는 캐나다 다국적 AI 스타트업 \\'코히어(Cohere)\\'에서 RAG팀을 이끌고 있는 패트릭 루이스(Patrick Lewis)는 2020년 한 논문 \\'지식 집약적 NLP 작업을 위한 검색 증강 생성(Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks-다운)\\'에서 RAG라는 용어를 처음 만들었다.\\n\\n그는 수백 편의 논문과 수십 개의 상용 서비스에서 이 용어가 생성\\xa0AI의 미래를 대표한다고 여겨지는 방법군을 설명하는 것에 대해 유감을 표시했다.\\n\\n패트릭 루이스는 지역 컨퍼런스에서 데이터베이스 개발자들과 자신의 아이디어를 공유한 싱가포르의 한 인터뷰 자리에서 \"우리의 연구가 이렇게 널리 퍼질 줄 알았다면 이름에 더 많은 고민을 했을 것이다. 항상 더 멋진 이름을 계획했지만, 막상 논문을 쓸 때가 되자 아무도 더 좋은 아이디어를 내놓지 못했다”고 말했다.\\n\\n그럼 RAG란 무엇인가?\\n\\nRAG는 외부 소스에서 가져온 정보로 생성\\xa0AI 모델의 정확성과 신뢰성을 향상시키는 기술이다.\\n\\n즉, LLM의 작동 방식에서 부족한 부분을 채워주는 기술이다. LLM은 내부적으로 신경망이며, 일반적으로 얼마나 많은 매개변수가 포함되어 있는지에 따라 측정된다. LLM의 매개변수는 기본적으로 인간이 단어를 사용해 문장을 구성하는 일반적인 패턴을 나타낸다.\\n\\n‘매개변수화된 지식’이라고도 하는 이러한 심층적인 이해 덕분에, LLM은 일반적인 프롬프트에 빠른 속도로 응답하는 데 유용하다. 그러나 최신 주제나 특정 주제에 대해 더 깊이 알고 싶어하는 사용자에게는 적합하지 않다.\\n\\n내부와 외부 리소스의 결합\\n\\n패트릭 루이스와 동료들은 RAG 기술을 개발해 생성 AI 서비스를 외부 리소스, 특히 최신 기술 정보가 풍부한 리소스에 연결했다.\\n\\n메타 AI 연구소(Meta AI Research), 유니버시티 칼리지 런던(University College London), 뉴욕대학교(New York University)의 공동 저자들이 참여한 논문(위 논문 참조)에서 RAG를 ‘범용 미세 조정 레시피’라고 칭했다. 이는 RAG가 거의 모든 LLM에서 모든 외부 리소스와 연결하는 데 사용할 수 있기 때문이다.\\n\\n사용자 신뢰 구축\\n\\nRAG는 연구 논문의 각주처럼 모델에 인용할 수 있는 소스를 제공한다. 따라서 사용자는 모든 출처를 확인할 수 있으며, 이는 사용자의 신뢰도를 강화한다. 또한, 이 기술은 모델이 사용자 쿼리의 모호함을 해소하는 데 도움이 된다. 즉, 모델이 잘못된 추측을 할 가능성인 환각 현상을 감소시킨다.\\n\\nRAG의 또 다른 큰 장점은 비교적 쉽다는 것이다. 패트릭 루이스와 논문 공동 저자 3명의 메타 블로그(보기)에 따르면, 개발자는 단 5줄의 코드(보기)만으로 이 프로세스를 구현할 수 있다(아래는 생성 예).\\n\\n따라서 추가 데이터 세트로 모델을 재훈련하는 것보다 더 빠르고 비용이 적게 든다. 또한 사용자가 새로운 소스를 즉시 핫스왑(hot-swap)할 수 있다.\\n\\n사람들이 RAG를 사용하는 방법\\n\\n사용자는 RAG를 통해 기본적으로 데이터 저장소와 대화할 수 있고, 새로운 경험을 하게 된다. 즉, RAG의 응용 분야는 사용 가능한 데이터 세트 수의 몇 배에 달한다.\\n\\n예를 들어, 의료 지수로 보강된 생성형 AI 모델은 의사나 간호사를 위한 훌륭한 보조 도구가 될 수 있다. 재무 분석가는 시장 데이터에 연결된 비서의 도움을 받을 수 있다.\\n\\n실제로 거의 모든 비즈니스에서 기술, 정책 매뉴얼, 동영상, 로그 등을 지식 베이스라는 리소스로 전환해 LLM을 향상시킬 수 있다. 이러한 리소스는 고객과 현장 지원, 직원 교육, 개발자 생산성 등의 분야에서 활용될 수 있다.\\n\\n이러한 광범위한 잠재력 때문에 엔비디아를 비롯한 구글(Google), 마이크로소프트(Microsoft), 아마존웹서비스(AWS-보기), IBM(보기), 글린(Glean-보기), 오라클(Oracle-보기), 파인콘(Pinecone-보기), 엘라스틱(Elastic-보기) 등\\xa0글로벌 빅테크 기업들이 RAG를 채택하고 있다.\\n\\n그 중에서 엔비디아의 사례를 살펴본다.\\xa0엔비디아는 사용자의 작업을 돕기 위해 RAG를 위한 레퍼런스 아키텍처(Reference Architecture-보기)를 개발했다. 여기에는 사용자가 이 새로운 방식으로 자체 애플리케이션을 만드는 데 필요한 요소와 샘플 챗봇이 포함된다.\\n\\n이 워크플로우에서는 생성\\xa0AI 모델을 개발하고 맞춤화하기 위한 엔비디아 네모(NeMo) 프레임워크와 제작 단계에서 생성\\xa0AI 모델을 실행하기 위한 엔비디아 트리톤 추론 서버(Triton Inference Server), 엔비디아 텐서RT-LLM(TensorRT-LLM)과 같은 소프트웨어가 사용된다.\\n\\n이 소프트웨어는 엔비디아 AI 엔터프라이즈(AI Enterprise)의 일부로 구성된다. 이는 비즈니스에 필요한 보안, 지원, 안정성을 갖춰 제작 준비가 된 AI의 개발과 배포를 가속화한다.\\n\\nRAG 워크플로우를 위한 최상의 성능을 얻으려면 데이터를 이동하고 처리하는 데 방대한 양의 메모리와 컴퓨팅이 필요하다. 288GB의 고속 HBM3e 메모리와 8페타플롭(Petaflops)의 컴퓨팅 성능을 갖춘 엔비디아 GH200 그레이스 호퍼 슈퍼칩(GH200 Grace Hopper Superchip)은 CPU를 사용할 때보다 150배 빠른 속도를 제공할 수 있는 이상적인 제품이다.\\n\\nRAG에 익숙해지면 다양한 기성 또는 맞춤형 LLM을 내부와 외부 지식 기반과 결합해 직원과 고객을 돕는 광범위한 보조 기능을 만들 수 있다.\\n\\nRAG는 데이터 센터를 따로 요구하지 않는다. 엔비디아 소프트웨어 덕분에 사용자는 노트북에서도 액세스할 수 있는 모든 애플리케이션을 지원해 윈도우 PC(Windows PC)에서 LLM을 사용할 수 있다.\\n\\n이제 엔비디아 RTX GPU가 탑재된 PC에서 일부 AI 모델을 실행할 수 있다. 사용자는 PC에서 RAG를 사용해 이메일, 메모, 기사 등 프라이빗 지식 소스에 연결하고 응답을 개선할 수 있다. 이를 통해 사용자는 데이터 소스, 프롬프트, 응답이 모두 비공개로 안전하게 유지된다는 확신을 가지게 된다.\\n\\n최근 블로그에서 더 나은 결과를 빠르게 얻기 위해 윈도우용 텐서RT-LLM으로 가속화된 RAG의 사례(보기)를 확인할 수 있다.\\n\\nRAG의 역사\\n\\n이 기술의 근원은 적어도 1970년대 초로 거슬러 올라간다. 정보 검색 분야의 연구원들이 처음에는 야구와 같은 좁은 주제에서 자연어 처리(NLP)를 사용해 텍스트에 액세스하는 앱인 질문-응답 시스템이라는 프로토타입을 만들었던 시기이다.\\n\\n이러한 종류의 텍스트 마이닝의 개념은 수년 동안 꾸준히 유지되어 왔다. 그러나 이를 구동하는 머신러닝 엔진은 크게 성장해 그 유용성과 인기가 높아졌다.\\n\\n1990년대 중반, 현재 애스크닷컴(Ask.com)인 애스크지브스(Ask Jeeves) 서비스는 잘 차려입은 마스코트 캐릭터로 질문 응답을 대중화했다. IBM의 왓슨(Watson)은 2011년 게임 쇼인 제퍼디(Jeopardy!)에서 인간 챔피언 두 명을 가볍게 꺾으며 TV 스타로 떠올랐다.\\n\\n오늘날 LLM은 질문 답변 시스템을 완전히 새로운 차원으로 발전시키고 있다.\\n\\n런던 연구소에서 얻은 인사이트\\n\\nRAG를 언급했을 2020년에 패트릭 루이스는 유니버시티 칼리지 런던에서 NLP 박사 학위를 취득하고 런던의 새로운 AI 연구소에서 메타를 위해 일하고 있었다. 연구팀은 LLM의 매개변수에 더 많은 지식을 담을 수 있는 방법을 찾고 있었고, 자체 개발한 벤치마크를 사용해 진행 상황을 측정하고 있었다.\\n\\n연구팀은 이전의 방법을 기반으로 구글 연구원의 논문(REALM: 검색-증강 언어 모델 사전 훈련-다운)에서 영감을 얻었다. 패트릭 루이스는 \"검색 인덱스가 중간에 있는 훈련된 시스템으로, 원하는 텍스트 출력을 학습하고 생성할 수 있다는 매력적인 비전을 갖고 있었다\"고 말했다.\\n\\n패트릭 루이스가 진행 중인 작업에 다른 메타 팀의 우수한 검색 시스템을 연결했을 때, 첫 번째 결과는 예상 외로 놀라웠다.\\n\\n그는 \"상사에게 결과를 보여 주었더니 아주 잘했다는 긍정적인 답변을 받았다. 이러한 워크플로우는 처음에 올바르게 설정하기 어려울 수 있어 이런 일은 자주 일어나지 않는다\"고 말했다.\\n\\n패트릭 루이스는 또한 당시 뉴욕대학교와 페이스북(Facebook) AI 리서치에 각각 재직 중이던 팀원 에단 페레즈(Ethan Perez)와 도위 키에라(Douwe Kiela)의 공로를 인정했다.\\n\\n엔비디아 GPU 클러스터에서 실행된 이 작업은 생성형 AI 모델을 더욱 정확하고 신뢰할 수 있게 만드는 방법을 보여줬다. 이후 이 연구는 수백 개의 논문에서 인용돼 개념을 확장했으며, 현재도 활발한 연구 분야로 자리 잡고 있다.\\n\\nRAG의 작동 방식\\n\\n사용자가 LLM에 질문을 하면 AI 모델은 기계가 읽을 수 있도록 쿼리를 숫자 형식으로 변환하는 다른 모델에 쿼리를 전송한다. 쿼리의 숫자 버전을 임베딩 또는 벡터라고 부른다.\\n\\n그런 다음 임베딩 모델은 이러한 숫자 값을 기계가 읽을 수 있는 지식 기반의 인덱스에서 벡터와 비교한다. 일치하는 항목이 하나 또는 여러 개 발견되면 관련 데이터를 검색해 사람이 읽을 수 있는 단어로 변환한 후 LLM에 다시 전송한다.\\n\\n마지막으로 LLM은 검색된 단어와 쿼리에 대한 자체 응답을 결합해 임베딩 모델이 찾은 소스를 인용하고 사용자에게 최종 답변을 제시한다.\\n\\n소스를 최신 상태로 유지\\n\\n임베딩 모델은 백그라운드에서 새롭고 업데이트된 지식창고를 사용할 수 있게 되면 벡터 데이터베이스라고도 하는 기계 판독 가능 인덱스를 지속적으로 생성하고 업데이트한다.\\n\\n많은 개발자들은 오픈 소스 라이브러리인 랭체인(LangChain)이 LLM을 연결하고 모델과 지식 기반을 통합하는 데 특히 유용하다고 생각한다. 엔비디아는 RAG를 위한 레퍼런스 아키텍처에서 랭체인을 사용한다.\\n\\n랭체인 커뮤니티는 RAG 프로세스에 대한 자체 설명을 제공하고 있다.\\n\\n한편, 생성형 AI의 미래는 모든 종류의 LLM과 지식 베이스를 창의적으로 연결해 사용자가 확인할 수 있고, 신뢰도 높은 결과를 제공하는 새로운 종류의 조수를 만든다. 엔비디아 런치패드 랩(LaunchPad Lab-보기)에서 AI 챗봇으로 RAG를 직접 사용할 수 있다.(아래는 머신러닝 스티리트 토크의\\xa0패트릭 루이스 박사와 \\'검색 증강 생성\\'에 대한\\xa0인터뷰 영상이다)\\n\\n\\n\\n(아래는 영상은 IBM의 검색 증강 생성(RAG)이란 무엇입니까?)\\n\\n\\n\\n기사 댓글\\n0\\n\\n비회원 로그인\\n\\n댓글쓰기\\n\\n하단영역\\n\\n하단메뉴\\n\\n매체정보\\n\\n전체메뉴\\n\\n</raw></document>', \"<document><title>Rag의 짧은 역사 훑어보기 (첫 논문부터 최근 동향까지)</title><url>https://pangyoalto.com/brief-history-rag/</url><content>RAG를 처음 제시한 논문은 Language model 및 retriever을 학습하는 방법으로 RAG를 제시하였다. 하지만 LLM의 대중화 이후 사람들이 사용하는 RAG의 의미는 LLM의 inference를 레버리지하는 방법으로 굳어지고 있다. ... 개발자들의 RAG 구축을 도와주기 위해 만든 구글의 Vertex AI</content><raw>RAG의 짧은 역사 훑어보기(첫 논문부터 최근 동향까지)\\n\\nPangyoalto\\n\\n[요약]\\n\\n들어가며\\n\\nChatGPT가 등장한 것이 22년 11월이니 대중들에게도 LLM이 친숙한 단어가 된 지 오랜 시간이 지났다. 그 사이 많은 기업들이 좋은 LLM을 만들고 활용하기 위해 노력을 기울였다.\\n\\nRAG는 LLM을 더 잘 활용하기 위한 기법이다. LLM은 학습한 시점 이후의 데이터가 없기 때문에 outdate된 정보를 줄 수 있고, domain specific한 지식이 부족한 단점이 있다. 주기적으로 LLM을 재학습시키면 되겠지만 이는 무척 비용이 크다. RAG는 LLM에 쿼리를 넘겨줄 때 데이터베이스에서 관련 정보를 찾아 같이 넘겨주면서 이를 극복한다.\\n\\n이번 글에서는 RAG라는 용어가 처음 등장한 Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks(2020)을 살펴볼 것이다. 그리고 Survey 논문(2023)을 참고하여 최근 RAG는 어떻게 발전되고 있는지 알아보고 필자가 인상깊게 본 구글의 Vertex AI를 간략하게 소개하고자 한다.\\n\\nRetrieval-Augmented Generation for Knowledge-Intensive NLP Tasks(2020)\\n\\nRAG 용어가 처음 등장한 Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks(2020) 간단하게 살펴보자.\\n\\n1) Overview\\n\\n이 논문에서 제시한 RAG 모델은 크게 Retriever와 Generator로 구성된다.\\n\\nRetriever은 위키피디아와 같은 데이터셋으로부터 쿼리에 알맞은 문서(latent documents)들을 \\x08가져온다(검색한다). 이때 데이터셋은 vector index를 가지고 있어 pretrained retriever가 문서를 가져올 수 있다.\\n\\nGenerator는 쿼리와 Retriever가 구한 문서들을 같이 받아 output을 생성한다. 이 논문에서는 Encoder-Decoder 모델인 BART를 사용하였다.\\n\\n해당 논문에서는 parametric knowledge와 non parametric knowledge라는 용어가 나온다. Parametric knowledge는 모델이 학습한 정보를 말하고 non parametric knowledge는 문서를 가져오는 외부 정보를 말한다.\\n\\n이 논문에서 실험한 것은 parametric memory를 가진 학습된 모델을 fine tuning할 때 non parametric memory를 활용한 것이다. RAG를 모델 fine tuning의 방법으로 제시한 것으로 볼 수 있다. 이는 우리가 현재 우리가 흔히 알고 있는 RAG와 약간 다르다. 해당 논문이 LLM이 본격적으로 등장하지 않은 2020년에 나왔기 때문에 language model을 학습시키는 방향으로 연구를 진행한 것 같다.\\n\\n2) Method\\n\\nRetriever: pretrained된 BERT의 encoder를 활용한 DPR(2019)을 기반으로 하였다.\\n\\nGenerator: pretrained된 BART(2019)를 사용하였다. Generator는 굳이 BART를 쓰지 않아도 되고, encoder-decoder 모델이면 다 가능하다.\\n\\n논문에서는 RAG의 모델을 두 가지 제시한다.\\n\\nRAG-Sequence 모델\\n\\nRAG-Token 모델\\n\\n위 모델을 조금 설명해보자면 RAG의 모델은 input sequence x로 문서들 z를 얻고, x와 z를 사용해 target sequence y를 생성한다.\\n\\nretriever은 쿼리 x에 대한 top K개의 문서들을 가져온다.\\n\\ngenerator는 이전 i-1개의 토큰들을 보고 새로운 토큰을 생성한다.\\n\\n수식을 보면 generator가 다음 토큰의 확률을 계산할 때 retriever가 해당 문서를 검색할 확률을 포함시키는 것을 알 수 있다. 이런 식으로 검색한 문서를 latent variable로 활용한다.\\n\\n앞서 말했듯이 retriever와 generator는 학습 대상이다. 여기서 document encoder를 학습 대상에서 제외하였는데, 주기적으로 문서 인덱스를 업데이트해야해 비용이 크고 성능에도 큰 영향을 주지 않기 때문이다.\\n\\n3) Experiment\\n\\n실험 대상 TASK는 다음 4가지다.\\n\\nOpen-domain Question Answering\\n\\nAbstractive Question Answering\\n\\nJeopardy Question Generation\\n\\nFact Verification\\n\\n각 task에 대해 non-parametic knowledge source로 위키피디아 dump를 동일하게 사용했고, 각 문서의 embedding으로 FAISS의 HNSW로 MIPS Index를 만들어 수행하였다.\\n\\n4) Evaluation\\n\\nOpen-domain QA의 경우 SOTA를 보였고, 그 외의 TASK에 대해서도 준수한 성능을 보였다.\\n\\n여기서 비교군으로 REALM(2020)이 나오는데 문서를 검색해서 모델을 학습시킨 선행 연구이다. REALM과 비교했을 때, document encoder를 학습시키지 않고 마스킹도 하지 않아 비용이 저렴하면서도 비슷하거나 좋은 성능을 냈다는 것에도 의의가 있다.\\n\\n최근 RAG 동향\\n\\nRetrieval-Augmented Generation for Large Language Models: A Survey을 참고하였다.\\n\\n앞서 살펴본 논문은 RAG를 처음 제시하였고 이를 통해 retriever과 generator을 개선하는 것을 목표로 하였다. 위 그림의 분류에서는 Fine-tuning에 해당된다.\\n\\n하지만 LLM이 유행하면서 RAG를 LLM의 능력을 레버리지하는 inference 단계에서 적용하는 경우가 늘어났다. LLM의 inference 단계에서 RAG를 적용하면 LLM의 단점인 hallucination 및 outdated knowledge를 생성을 완화할 수 있다.\\n\\nRAG의 단계는 크게 3가지로 나눌 수 있다.\\n\\n1) Indexing: 문서들을 청크로 분리 및 벡터로 인코딩하여 벡터 데이터베이스에 저장\\n\\n2) Retrieval: Query와 관련된 Top K개의 chunk를 가져옴\\n\\n3) Generation: 원본 Query와 Retrieval로 가져온 chunk들을 LLM에 input으로 넣어 최종 답변 생성\\n\\nRAG 패러다임\\n\\n해당 Survey 논문에서는 RAG를 3가지 카테고리로 나누었다.\\n\\nRAG 연구 자체가 시작된지 얼마 안되었고 Modular RAG는 특히 초기 단계인 것으로 보인다. 기업에서 적용하고 있는 RAG는 Naive RAG 혹은 Advanced RAG가 주이다.\\n\\n1) Naive RAG\\n\\nChatGPT가 광범위하게 사용되면서 같이 널리 사용된 방법이다. Naive RAG는 indexing, retrieval, generation을 포함한 전통적인 방법을 사용한다.\\n\\nIndexing - PDF, HTML, Word, Markdown 등의 다양한 포맷으로부터 raw data를 추출하고 일관된 text 포맷으로 저장한다. 먼저 text를 더 작고 이해하기 쉬운 chunk들로 쪼개고 이를 임베딩 모델을 통해 vector로 인코딩해 벡터 데이터베이스에 저장한다.\\n\\nRetrieval - RAG 시스템은 쿼리도 vector로 변환시킨다. 이때 indexing phase에서 사용한 같은 인코딩 모델을 사용한다. 이를 통해 변환된 query 벡터와 indexing 벡터의 similarity score를 계산하여 top K개의 chunk를 가져온다.\\n\\nGeneration - 쿼리와 Retrieval 단계에서 선택된 문서들을 합쳐 프롬프트로 만들고 language model에서 사용한다.\\n\\n그러나 Naive RAG는 여러 단점들이 존재한다.\\n\\n2) Advanced RAG\\n\\nNative RAG에서 Retrieval 질을 향상시키기 위해 pre-retrieval과 post-retrieval을 추가로 사용한다.\\n\\nPre-retrieval process\\n\\nPost-Retrieval Process\\n\\n검색한 context들을 query와 효율적으로 합치는 방법을 개선하는 작업이다. LLM의 부하를 줄이기 위해 필수 정보를 선택, 중요 섹션을 강조, context를 요약 작업 등이 있다.\\n\\n3) Modular RAG\\n\\n앞선 두 패러다임을 넘어 발전된 adaptability을 제공한다.\\n\\nsimilarity search를 위한 검색 모듈을 추가한다던가 fine tuning을 통해 retreiver을 정제한다던가 등의 작업을 “모듈”의 형태 혹은 “패턴”의 형태로 제공하려는 것을 시도하는 것 같다.\\n\\n초기 연구 단계이고 자세히 살펴볼 만한 부분은 없어 생략하겠다.\\n\\nRAG에서 중요한 것: Retrieval\\n\\nRAG는 비용이나 시간적인 문제로 LLM을 Fine tuning하기 어렵기 때문에 선택하는 경우가 많다. 따라서 같은 LLM을 사용하더라도 inference 성능을 잘 레버리지 해야한다.\\n\\n따라서 RAG에서 통제할 수 있는 것은 Retrieval로 가져오는 문서 뿐이다. 그렇기에 1) 어떻게 문서를 잘 가져올 것이며 2) 가져온 문서들을 쿼리와 조합하여 LLM에 어떻게 넘길 것인지가 중요하다.\\n\\n문서를 잘 가져오는 방법은 다음과 같은 것들이 있다.\\n\\n가져온 문서들을 쿼리와 조합할 때 시도해볼 것은 크게 두 가지가 있다.\\n\\n첫번째는 문서들을 다시 랭킹(Reranking)하는 것이다. 랭킹 기준에는 전통적인 Rule based 방법인 Relevance나 MRR 등을 사용할 수도 있고 model based인 BERT 등을 사용할 수도 있다.\\n\\n두번째는 Context에서 중요한 부분만 선택하거나 압축하는 방법입니다. LLM도 사람과 비슷하여 처음과 끝에 집중하는 경향이 있다. 그래서 context가 너무 길어지면 중간 정보를 빠뜨리는 문제가 발생한다.\\n\\n사실 여기서 얘기한 Retrieval을 잘하는 방법들은 이미 검색 관련 분야에서 이미 연구 및 적용 중이다. 그래서 검색의 패러다임이 생성형 검색으로 넘어가도 현재 검색 엔진을 잘하는 기업이 잘할 가능성이 높다는 생각이 든다.\\n\\n정리하며\\n\\nRAG를 처음 제시한 논문은 Language model 및 retriever을 학습하는 방법으로 RAG를 제시하였다. 하지만 LLM의 대중화 이후 사람들이 사용하는 RAG의 의미는 LLM의 inference를 레버리지하는 방법으로 굳어지고 있다.\\n\\nRAG는 1) 문서를 잘 검색하여 2) 원본 query와 함께 LLM에 잘 넣는 것이 중요합니다. 이를 위한 방법으로 Indexing이나 query 최적화, reranking, context 압축 등의 연구가 진행 중이다.\\n\\nRAG에서 흔히 쓰이는 DB는 Pinecone이나 Milvus와 같은 벡터 데이터베이스이다. 또 Langchain처럼 Retrieval을 구현하기 쉽게 도와주는 framework들도 존재한다.\\n\\n벡터 데이터베이스에서 Retrieval은 벡터 간 거리를 바탕으로 검색하는 similarity search입니다. 하지만 이는 한계가 있을 수 있다. 예를 들어 새로운 분야에 대해서는 잘 작동하지 않을 수 있고, 금융이나 헬스케어와 같은 산업에서는 정확한 키워드를 기반한 검색 결과가 중요하다.\\n\\n개발자들의 RAG 구축을 도와주기 위해 만든 구글의 Vertex AI는 벡터 검색의 한계를 극복하기 위해 키워드 검색까지 포함시켜 제공해준다.\\n\\n키워드 검색 뿐만 아니라 Vertex AI는 LLM reasoning을 통해 기존 쿼리에서 다른 쿼리들로 확장시킨다. 예를 들어 “warm clothing for winter”라는 쿼리가 들어오면 이를 LLM이 “Warm winter jacket”, “Ski clothing”, “Cashmere sweaters” 등으로 쿼리를 생성해 여러 쿼리를 수행한다.\\n\\n이러한 제품들의 발전 방향을 보면 초기 사람들이 생각했던 구글의 검색 점유율 하락은 앞으로도 크지 않을 것으로 보인다. 여전히 전통적인 문서 검색 방법도 중요하고 계속 쓰일 것이기 때문이다. 앞으로 구글을 비롯한 최고의 검색 기술을 가지고 있는 회사가 어떻게 RAG를 활용할지 지켜보는 것도 재밌을 것 같다.\\n\\nReference\\n\\nRetrieval-Augmented Generation for Knowledge-Intensive NLP Tasks(2020)\\n\\nRetrieval-Augmented Generation for Large Language Models: A Survey(2023)\\n\\nYour RAGs powered by Google Search technology\\n\\nA guide to the LLM Agent Framework for the average developer\\n\\n평범한 개발자들을 위한 LLM 에이전트 프레임워크 가이드\\n\\nDon't trust Go GC too much - detecting memory leaks and managing GC cycles\\n\\n</raw></document>\", '<document><title>Rag란 무엇입니까? 주목 받은 이유 및 미래 트렌드</title><url>https://hblabgroup.com/ko/rag란-무엇입니까-주목-받은-이유-및-미래-트렌드/</url><content>rag를 설계할 때 자주 범하는 오류는 \\\\\"데이터만 있으면 쓸 수 있다\\\\\"는 가정입니다. 하지만 실제 조직에서는 내부 데이터라도 부서 간 접근 제한이 걸려 있거나, nda, gdpr, hipaa 등 법적 제한으로 인해 활용이 금지된 경우가 많습니다.</content><raw>RAG란 무엇입니까? 주목 받은 이유 및 미래 트렌드\\n\\n목차\\n\\nRAG란 검색 보강 생성(Retrieval-augmented generation, RAG) 약자로 대형 언어 모델(LLM)의 출력이 특정 맥락에서 더 관련성이 높도록 만드는 과정입니다.\\n\\n급변하는 디지털 시대, 기업들은 방대한 데이터를 어떻게 활용하고 어떻게 더 정확한 정보를 사용자에게 전달할 수 있을지 끊임없이 고민하고 있습니다. 기존의 언어 모델은 정형화된 데이터나 학습된 정보에 의존하는 한계가 있었고 이로 인해 최신 정보나 특정 도메인 지식에 기반한 답변에는 취약한 모습을 보이기도 했습니다.\\n\\n이러한 한계를 극복하기 위한 솔루션으로 최근 주목받고 있는 기술이 바로 RAG(Retrieval-Augmented Generation)입니다. 검색 기반 생성 기술인 RAG는 정적 모델이 아닌, 동적으로 외부 지식을 검색하여 보다 정확하고 신뢰도 높은 응답을 생성할 수 있도록 지원합니다.\\n\\n이 글에서는 RAG의 개념부터 구조, 장점과 활용 사례, 그리고 이를 성공적으로 적용하기 위한 고려사항까지 폭넓게 살펴보고자 합니다. 또한, RAG 기술을 성공적으로 도입하고 싶은 기업에게 베트남의 유망 AI 아웃소싱 파트너인 HBLAB과의 협업 가능성도 함께 제안드립니다.\\n\\nRAG란 무엇입니까?\\n\\nRAG란 검색 보강 생성(Retrieval-augmented generation, RAG) 약자로 대형 언어 모델(LLM)의 출력이 특정 맥락에서 더 관련성이 높도록 만드는 과정입니다. RAG는 인공지능(AI) 모델의 성능을 최적화하기 위해 외부 지식 기반과 연결하는 아키텍처이고 대형 언어 모델(LLM)이 더 관련성 높은 응답을 더 높은 품질로 제공하는 데 도움을 줍니다.\\n\\n생성형 AI(Generative AI) 모델은 대규모 데이터셋에서 훈련되며 이 정보를 참조하여 출력을 생성합니다. 그러나 훈련 데이터셋은 유한하며 AI 개발자가 접근할 수 있는 정보, 공공 도메인 작품, 인터넷 기사, 소셜 미디어 콘텐츠 및 기타 공개적으로 접근 가능한 데이터에 제한됩니다.\\n\\nRAG는 생성형 AI 모델이 내부 조직 데이터, 학술 저널 및 전문 데이터셋과 같은 추가 외부 지식 기반에 접근할 수 있게 합니다. 관련 정보를 생성 과정에 통합함으로써 챗봇 및 기타 자연어 처리(NLP) 도구는 추가 훈련 없이도 더 정확한 도메인 특정 콘텐츠를 생성할 수 있습니다.\\n\\nRAG는 어떻게 작업합니까?\\n\\nRAG는 언어 모델에 필요한 정보를 제공하는 것입니다. 일반적인 모델처럼 LLM에 직접 질문하는 대신, 먼저 잘 관리된 지식 라이브러리에서 매우 정확한 데이터를 검색하고 나서 그 맥락을 사용하여 답변을 반환합니다. 사용자가 검색기에게 쿼리(질문)를 보내면 벡터 임베딩(숫자 표현)을 사용하여 요청된 문서를 검색합니다.\\n\\n필요한 정보가 벡터 데이터베이스에서 발견되면 결과가 사용자에게 반환됩니다. 이는 환각 가능성을 크게 줄이고 모델을 재훈련하지 않고 업데이트할 수 있어 비용이 많이 드는 프로세스를 피할 수 있습니다.\\n\\n구체적으로 설명하자면 다음과 같습니다.\\n\\n외부 데이터 생성\\n\\nLLM의 원래 훈련 데이터 세트 외부에서 생성된 새로운 데이터를 외부 데이터라고 합니다. 이는 API, 데이터베이스 또는 문서 저장소와 같은 여러 데이터 소스에서 올 수 있습니다. 데이터는 파일, 데이터베이스 레코드 또는 장문의 텍스트와 같은 다양한 형식으로 존재할 수 있습니다. 임베딩 언어 모델이라는 또 다른 AI 기술은 데이터를 숫자 표현으로 변환하고 이를 벡터 데이터베이스에 저장합니다. 이 과정은 생성적 AI 모델이 이해할 수 있는 지식 라이브러리를 만듭니다.\\n\\n관련 정보 검색\\n\\n관련 정보 검색 단계는 관련성 검색을 수행하는 것입니다. 사용자 쿼리는 벡터 표현으로 변환되어 벡터 데이터베이스와 일치합니다. 예를 들어, 회사의 기술 지원 질문에 답할 수 있는 스마트 챗봇을 생각해 보십시오. 직원이 ‘비밀번호를 잊어버렸어요’라고 요청하면, 시스템은 해당 직원의 계정 정보와 비밀번호 복구 절차를 함께 검색하여 즉시 해결 방법을 제공합니다.\\n\\n이러한 특정 문서는 직원이 입력한 내용과 매우 관련성이 높기 때문에 반환됩니다. 관련성은 수학적 벡터 계산과 표현을 사용하여 계산되고 설정됩니다.\\n\\nLLM 프롬프트 보강\\n\\nRAG 모델은 관련 검색 데이터를 맥락에 추가하여 사용자 입력(또는 프롬프트)을 보강합니다. 이 단계는 LLM과 효과적으로 소통하기 위해 프롬프트 엔지니어링 기법을 사용합니다. 보강된 프롬프트는 대형 언어 모델이 사용자 쿼리에 대한 정확한 답변을 생성할 수 있도록 합니다.\\n\\n외부 데이터 업데이트\\n\\n다음 질문은 외부 데이터가 stale(구식)이 되면 어떻게 할 것인가입니다. 검색을 위한 현재 정보를 유지하기 위해 문서를 비동기적으로 업데이트하고 문서의 임베딩 표현을 업데이트해야 합니다. 이는 자동화된 실시간 프로세스 또는 주기적인 배치 처리를 통해 수행할 수 있습니다. 이는 데이터 분석에서 일반적인 도전 과제로, 변화 관리에 대한 다양한 데이터 과학 접근 방식이 사용될 수 있습니다.\\n\\n현재\\xa0RAG는 어떻게 사용됩니까?\\n\\n가상 비서\\n\\nRAG는 가상 비서 및 챗봇을 생성하여 최신 이벤트, 날씨 및 뉴스를 검색하고 사용자 질문에 자연스러운 언어로 답변할 수 있습니다. 검색 모델은 지식베이스에서 관련 정보를 검색하고 생성 모델은 문맥에 맞는 정확하고 유창한 응답을 생성하여 사용자 경험을 향상시킵니다. RAG 덕분에 가상 비서는 사용자의 요구 사항을 더 잘 이해하고 더 유연하게 응답할 수 있습니다.\\n\\n예를 들어, 사용자가 날씨 정보를 요청하면 가상 비서는 최신 날씨 데이터를 검색하여 실시간 예보를 정확히 제공할 수 있습니다. 또한 RAG는 챗봇이 더 복잡한 질문을 처리하고 더 스마트한 답변을 제공할 수 있도록 도와줍니다. 이렇게 함으로써 시간을 절약하고 업무 효율성을 높일 수 있습니다.\\n\\n질문 응답 시스템\\n\\nRAG는 질문 응답 시스템에서 관련 문서나 구절을 검색하고 생성 모델이 이를 기반으로 상세하고 일관된 답변을 생성하는 데 사용됩니다. 이 시스템은 기존의 문서에서 답을 추출하는 대신, 주어진 정보로 새로운 텍스트를 생성하여 정확하고 자연스러운 답변을 제공합니다.\\n\\n의료나 교육 분야에서 RAG는 질병의 증상이나 강의 내용을 상세히 설명하는 데 사용될 수 있습니다. 예를 들어, 사용자가 특정 질병의 증상에 대해 질문하면 RAG는 의료 데이터베이스를 검색하여 구체적이고 정확한 답변을 생성할 수 있습니다.\\n\\n스마트 검색 시스템\\n\\nRAG는 전통적인 검색 모델과 생성 모델의 문맥 생성 능력을 결합하여 검색 기능을 향상시킵니다. 사용자가 정보를 검색하면 시스템은 데이터를 조회하고 즉시 답변을 생성하여 관련 문서를 나열하는 대신 직접적인 답변을 제공합니다. 이를 통해 사용자는 정보 검색의 시간을 단축할 수 있습니다.\\n\\n예를 들어, 온라인 검색 엔진에서 RAG는 즉시 간결하고 정확한 답변을 제공하여 사용자가 여러 검색 결과를 살펴보는 시간을 절약할 수 있습니다. 이는 고객 지원이나 기술 문제 해결과 같은 분야에서 특히 유용합니다.\\n\\n자동 콘텐츠 생성\\n\\nRAG는 블로그 글, 마케팅 자료 등 자동 콘텐츠 생성에 도움을 줄 수 있습니다. 특정 주제나 질문을 제공하면 시스템은 기존 정보원을 검색하고 적합한 글을 자동으로 생성합니다. 이를 통해 콘텐츠 제작자는 시간을 절약하고 품질 높은 글을 손쉽게 작성할 수 있습니다.\\n\\n예를 들어, 마케팅 분야에서는 RAG가 소셜 미디어 게시물, 이메일 마케팅, 제품 광고 콘텐츠를 생성하는 데 활용될 수 있습니다. 관리자는 기본 정보를 제공하기만 하면 시스템은 매력적이고 효과적인 마케팅 메시지를 자동으로 생성합니다.\\n\\n번역 지원\\n\\nRAG는 검색 모델과 생성 모델을 결합하여 자동 번역의 품질을 개선할 수 있습니다. 문장이나 구절을 번역해야 할 때, 모델은 관련 단어나 구를 검색하고 자연스럽고 정확한 번역을 생성합니다. 이를 통해 번역 품질을 개선하고 번역 오류를 최소화할 수 있습니다.\\n\\n예를 들어, 온라인 번역 서비스에서는 RAG가 보다 정확한 번역을 생성하는 데 사용될 수 있습니다. 특히 의료, 기술, 법률 등 전문 분야에서 더 빠르고 효율적인 번역을 제공할 수 있습니다.\\n\\n자동 코드 생성\\n\\nRAG는 기존 코드 샘플을 검색하고 이를 기반으로 특정 애플리케이션에 필요한 코드를 생성하여 자동 코드 작성을 도울 수 있습니다. 이는 소프트웨어 또는 애플리케이션 개발에 유용하며, 프로그래머가 시간을 절약하고 코드 오류를 줄이는 데 도움을 줍니다.\\n\\n예를 들어, 애플리케이션 개발 중 RAG는 사용자의 요구 사항에 맞춰 완성된 코드를 자동으로 생성하여 프로그래머가 처음부터 다시 작성할 필요 없이 프로젝트에 쉽게 통합할 수 있습니다. 이를 통해 성능이 향상되고 개발 비용이 절감됩니다.\\n\\n자동 마케팅 콘텐츠 생성\\n\\nRAG는 광고, 소셜 미디어 게시물, 이메일 마케팅 등 마케팅 콘텐츠를 자동으로 생성할 수 있습니다. 이 모델은 데이터를 검색하고 고객의 요구나 특정 캠페인에 맞는 마케팅 메시지를 생성합니다. 이를 통해 시간을 절약하고 마케팅 캠페인의 효과를 높일 수 있습니다.\\n\\n예를 들어, 소매업에서 RAG는 매력적인 광고를 생성하여 판매를 촉진하고 더 효과적인 마케팅 전략을 구축하는 데 도움을 줄 수 있습니다. 기업은 콘텐츠 창작 과정을 자동화하여 마케팅 비용을 절감할 수 있습니다.\\n\\n생산 프로세스 최적화\\n\\nRAG는 관련 데이터를 검색하고 효율적인 생산 계획을 자동으로 생성하여 생산 프로세스를 최적화하는 데 활용될 수 있습니다. 이 시스템은 여러 출처의 정보를 사용하여 최상의 생산 계획을 생성하여 기업의 시간과 비용을 절감하는 데 도움을 줍니다.\\n\\n예를 들어, 자동차 산업에서는 RAG가 최적의 생산 일정을 생성하여 제조업체가 비용을 절감하고 생산성을 높이는 데 도움을 줄 수 있습니다. 이는 재고 관리와 공급망 개선에도 기여합니다.\\n\\n프로젝트 관리\\n\\nRAG는 프로젝트 관리에서 상세한 프로젝트 계획과 진행 보고서를 생성하는 데 활용될 수 있습니다. 시스템은 프로젝트 데이터베이스에서 정보를 조회하고 최신 정확한 정보를 바탕으로 자동으로 문서를 생성합니다.\\n\\n예를 들어, 건설 프로젝트에서 RAG는 진행 상황과 발생한 문제에 대한 보고서를 생성하여 관리자가 계획을 쉽게 추적하고 조정할 수 있도록 도와줍니다. 이를 통해 시간 절약과 프로젝트 모니터링 효율성을 높일 수 있습니다.\\n\\n학습 도구 콘텐츠 생성\\n\\nRAG는 자동으로 강의 자료, 시험지 또는 복습 질문을 생성하여 학습 도구 콘텐츠를 만들 수 있습니다. 이 시스템은 기존 교육 자료를 조회하고 학생의 요구에 맞는 학습 자료를 생성합니다. 이를 통해 교사는 강의 자료와 시험 문제를 준비하는 시간을 절약할 수 있습니다.\\n\\n예를 들어, 교육 분야에서는 RAG가 다양한 학습 자료를 자동으로 생성하여 학생들이 효율적으로 복습할 수 있도록 도와줍니다.\\n\\nRAG 도입의 이점\\n\\nAI 도입 및 확장 비용 절감\\n\\n기존의 대규모 언어 모델(LLM)을 새로운 정보로 업데이트하거나 특정 산업에 맞게 조정하려면, 막대한 비용과 시간이 소요됩니다. 하지만 RAG는 외부 지식 소스(사내 문서, 웹사이트, 고객 보고서 등)를 실시간으로 불러와 활용할 수 있어, 모델 자체를 재학습하지 않고도 정보를 업데이트할 수 있습니다. 이로써 운영 비용이 크게 줄어듭니다.\\n\\n예를 들어, 한 금융회사가 새로운 수수료 정책이나 사내 규정을 업데이트할 경우, RAG는 해당 내용을 반영한 문서를 바로 연결해 응답에 활용할 수 있습니다. 모델을 다시 학습시킬 필요 없이 문서만 수정하면 되므로 관리도 간편합니다.\\n\\n비용 절감 외에도 빠른 서비스 도입과 반복 실험이 가능하다는 점에서, 초기 구축 비용 부담 없이 기업이 빠르게 AI를 현장에 적용할 수 있는 장점이 있습니다.\\n\\n최신 데이터 및 도메인 특화 지식에 접근 가능\\n\\n기존 LLM은 훈련 당시의 데이터만 반영하므로, 최신 정보나 전문 영역의 지식에는 대응하기 어렵습니다. 반면 RAG는 실시간으로 외부 지식 소스를 검색하여, 가장 최신의, 특정 도메인에 맞춤화된 정보 기반 응답을 생성할 수 있습니다.\\n\\n의료, 법률, 금융 등 정보가 자주 바뀌고 정확성이 중요한 분야에서 특히 유용합니다. 예컨대 새로운 의료 가이드라인이 발표되면 RAG는 해당 문서를 즉시 반영하여 AI가 최신 기준에 맞게 응답할 수 있게 합니다.\\n\\n또한 기업 내부 시스템(API, 문서 관리 시스템 등)과 연동하면, 사용자는 AI를 통해 실시간 정보에 기반한 정확한 답변을 얻을 수 있습니다. 이는 AI가 단순 텍스트 생성기를 넘어 실제 업무 지식을 가진 조력자가 되게 합니다.\\n\\nAI 환각 현상 방지\\n\\nLLM의 주요 문제 중 하나는 ‘환각’ 현상입니다. 즉, AI가 그럴듯하지만 사실이 아닌 정보를 자신감 있게 생성하는 경우입니다. RAG는 외부에서 실제 문서를 검색해 그 내용을 기반으로 응답을 생성하기 때문에, 이런 오류 가능성을 줄일 수 있습니다.\\n\\n예를 들어 사용자가 사내 연차 규정에 대해 질문하면, RAG는 해당 내용을 포함한 인사정책 문서를 검색해 응답합니다. 추측이 아닌 실제 문서 기반 응답이기 때문에 정확하고 검증 가능합니다.\\n\\n특히 법률이나 의료처럼 잘못된 정보가 심각한 결과를 초래할 수 있는 분야에서는, RAG의 사실 기반 응답 구조가 필수적입니다. 이는 기업이 신뢰할 수 있는 AI 시스템을 구축하는 데 중요한 조건입니다.\\n\\n사용자의 신뢰와 투명성 확보\\n\\nAI의 응답이 아무리 자연스럽더라도, 출처가 불분명하거나 근거가 없는 경우 신뢰하기 어렵습니다. RAG는 답변에 사용된 문서의 출처나 인용문을 함께 제공함으로써, 사용자가 스스로 정보를 검증할 수 있게 도와줍니다.\\n\\n예를 들어 “2024년 4월 10일자 인사정책에 따르면…”처럼 명확한 출처를 포함하면, 사용자는 AI의 응답을 단순히 수용하는 것이 아니라, 그 신뢰도를 판단할 수 있습니다. 이는 특히 법률 자문, 고객 응대, 의료 상담 등에서 매우 중요합니다.\\n\\n결국 RAG는 단순히 “똑똑한 AI”를 만드는 것이 아니라, 책임 있는 AI, 신뢰받는 AI를 만드는 기반이 됩니다. 이는 AI의 실무 도입을 가속화하는 핵심 조건 중 하나입니다.\\n\\n다양한 산업에 AI 활용 사례 확장\\n\\n기존에는 AI가 데이터가 많고 구조화된 영역에만 사용되었다면 RAG를 활용하면 데이터가 빠르게 바뀌거나 복잡한 문맥을 가진 산업에서도 AI를 효과적으로 적용할 수 있습니다.\\n\\n예를 들어,\\n\\n이처럼 RAG는 AI가 단순히 “말 잘하는 모델”을 넘어서, 실제 기업의 지식을 연결하고, 실무 현장에서 믿고 쓸 수 있는 지능형 시스템으로 발전하는 기반이 됩니다.\\n\\n개발자와 기업의 운영 제어권 향상\\n\\n기존 LLM에서는 AI의 응답을 바꾸기 위해선 모델 전체를 재학습하거나, 파인튜닝하는 과정이 필요했습니다. 하지만 RAG는 검색 대상이 되는 문서만 수정하면, 곧바로 AI의 응답이 달라집니다. 이로써 개발자와 도메인 전문가 모두 쉽게 관리할 수 있습니다.\\n\\n예컨대 인사팀은 단순히 “2024 연차규정.pdf” 문서만 업데이트하면, 모델 응답이 자동으로 변경됩니다. IT팀이 모델 코드를 수정하거나 재배포할 필요가 없습니다.\\n\\n또한 보안이나 민감 정보 제어도 유리합니다. RAG는 접근 가능한 지식 범위를 명확히 설정할 수 있어, AI가 불필요하거나 민감한 정보를 생성하지 않도록 제어할 수 있습니다. 이는 금융, 의료, 공공기관 등에서 매우 큰 장점입니다.\\n\\nRAG의 화려함 뒤에 숨겨진 잠재적 과제\\n\\nRetrieval-Augmented Generation(RAG)은 생성형 AI에 실제 지식 기반을 결합함으로써, 기업 현장에서 신뢰할 수 있는 AI 구현을 가능케 하는 획기적인 기술로 주목받고 있습니다. LLM의 주요 한계였던 환각(hallucination), 지속적 업데이트의 어려움, 파인튜닝 비용 등을 해결하며, 다양한 산업군에서 빠르게 도입되고 있죠.\\n\\n하지만 그 가능성 뒤에는 간과되기 쉬운 데이터 품질, 포맷 다양성, 접근 권한 및 라이선스 문제라는 구조적 도전들이 존재합니다. 지금부터 살펴볼 세 가지 과제는 기술 자체보다는 “데이터”에 집중해야 한다는 점에서 RAG의 본질을 다시 생각하게 합니다.\\n\\n데이터 품질\\n\\nRAG의 핵심 강점은 실제 문서를 기반으로 답변을 생성한다는 점입니다. 하지만 그 ‘실제 문서’가 오래되었거나, 오류가 많거나, 구조가 비정형적이라면 오히려 잘못된 정보를 전달할 수 있습니다.\\n\\n예를 들어, 고객이 환불 정책을 문의했을 때 RAG가 2019년도 구형 문서를 참조해 응답한다면, 이는 단순한 정보 오류를 넘어서 브랜드 신뢰도 저하 및 컴플레인, 법적 분쟁으로까지 이어질 수 있습니다.\\n\\n또한 문서가 PDF 스캔본이거나, 표/이미지가 많은 비정형 포맷이라면 검색 정확도는 급격히 떨어집니다. RAG가 성능을 발휘하려면 정돈된 문서 구조와 명확한 문맥, 최신성이 확보되어야 합니다. 즉, AI보다 먼저 데이터가 ‘일할 수 있는 상태’여야 합니다.\\n\\n멀티모달 데이터\\n\\n현실의 업무 문서는 텍스트만 존재하지 않습니다. 특히 제조, 의료, 건설, 교육 등 다양한 산업군에서는 도면, 이미지, 영상, 음성, 프레젠테이션 등 다양한 형태로 지식이 저장됩니다.\\n\\n예를 들어, 제조 현장의 장비 매뉴얼이 그림 위주로 되어 있거나, 병원의 진료기록이 CT 이미지와 같이 저장되어 있다면, 기존 RAG는 이를 처리할 수 없어 맹점이 발생합니다. 이는 AI가 충분한 정보를 가지고 있음에도 “이해하지 못하고 넘기는” 상황을 야기합니다.\\n\\n이러한 문제를 해결하려면, 단순 텍스트 기반의 RAG에서 나아가 멀티모달 RAG로의 전환이 필요합니다. 이미지 인식, OCR, 영상 분석 모델 등과 연동하여 다양한 형태의 정보를 벡터화하거나, 적절한 메타데이터로 구조화해 활용할 수 있어야 합니다. 하지만 이는 높은 기술력과 체계적인 데이터 파이프라인이 전제되어야 가능한 영역입니다.\\n\\n접근 권한과 라이선스\\n\\nRAG를 설계할 때 자주 범하는 오류는 “데이터만 있으면 쓸 수 있다”는 가정입니다. 하지만 실제 조직에서는 내부 데이터라도 부서 간 접근 제한이 걸려 있거나, NDA, GDPR, HIPAA 등 법적 제한으로 인해 활용이 금지된 경우가 많습니다.\\n\\n또한 외부 데이터를 사용하는 경우에는 라이선스 문제가 민감하게 작용합니다. 위키, 언론, 리서치 문서 등은 상업적 사용이 제한되거나 인용이 필요한 경우가 있어, 무단 사용 시 법적 리스크가 발생할 수 있습니다.\\n\\n따라서 RAG 시스템을 설계할 때는 기술 팀뿐 아니라 법무팀과 보안팀의 협업이 반드시 필요하며, 데이터는 사용 목적에 따라 라이선스 범주(공개, 내부 전용, 상업 불가 등)를 명확히 분류하고, 검색 가능한 문서 범위를 권한별로 제한하는 구조가 필요합니다.\\n\\nRAG는 매우 강력한 기술입니다. 그러나 GPT-4, Claude, Mistral 등 어떤 고성능 LLM을 활용하더라도, RAG가 의존하는 지식의 품질과 접근성, 라이선스 적합성이 확보되지 않으면 결국 불안정한 시스템이 되고 맙니다.\\n\\nRAG 프로젝트는 단순한 AI 도입이 아닙니다. 이는 곧 지식의 구조화, 접근성 통제, 그리고 거버넌스 설계라는 조직 차원의 전략적 과제입니다.\\n\\n결국, AI가 진짜 똑똑해지기 위해선 먼저 조직의 지식 인프라가 ‘똑똑하게’ 준비되어 있어야 합니다. 이것이 RAG 성공의 핵심입니다.\\n\\nRAG에 대한 미래 트렌드 예측\\n\\n실시간 RAG\\n\\nAI 시스템은 실시간 데이터 피드를 RAG 모델에 통합하여 최신 정보를 동적으로 검색할 수 있게 됩니다. 실시간 RAG는 외부 지식베이스, 웹사이트, 구조화된 데이터 소스와 연결하여 생성형 AI 솔루션이 정확하고 맥락에 적합한 자료를 제공하도록 보장합니다.\\n\\n기업들은 이 기능을 활용해 고객 참여를 향상시키고, 특히 지속적인 데이터 업데이트가 필요한 분야에서 의사결정을 개선할 것입니다.\\n\\n멀티모달 콘텐츠 (Mutilmodal content)\\n\\nRAG는 텍스트 기반 검색을 넘어 사진, 비디오, 오디오를 포함하는 보다 포괄적인 AI 구동 경험으로 발전할 것입니다. 벡터 데이터베이스와 하이브리드 검색 기법을 활용해 AI 시스템은 다양한 외부 소스에서 데이터를 평가하고 검색할 수 있게 됩니다.\\n\\n이 혁신을 바탕으로 전반적인 사용자 검색 경험을 개선하고 AI가 다양한 정보 형식에 적응하는 능력을 확장하는 데 도움이 됩니다.\\n\\n하이브리드 모델 (Hybrid model)\\n\\n키워드 검색과 지식 그래프, 의미 기반 검색 같은 고급 검색 기법을 결합해 검색 과정을 최적화합니다. 다양한 데이터 소스에서 관련 문서를 확보하고 검색 결과를 개선하며 응답 정확도를 높임으로써 하이브리드 모델은 AI 애플리케이션을 향상시킵니다.\\n\\n맞춤형 RAG 구현\\n\\nFew-shot 프롬프팅, 저랭크 적응(LoRA) 같은 파인튜닝 기술의 발전으로 AI 모델은 매우 개인화된 콘텐츠를 검색하고 생성할 수 있게 됩니다. 맞춤형 RAG는 고객 상호작용을 개선하고 맥락에 맞는 관련 데이터를 획득하며 사용자 질문을 정교화합니다.\\n\\nAI 기반 고객 서비스, 맞춤형 추천, 적응형 학습 시스템 등에서 큰 혜택을 기대할 수 있습니다.\\n\\n희소성 기법\\n\\n희소 검색 모델과 효율적인 데이터 아키텍처가 검색 시스템을 개선하여 처리 비용을 낮추고 검색 속도를 높입니다. 이러한 기법은 보안, 의료, 금융 등 대규모 분야에서 신속한 정보 검색이 필수적인 AI 애플리케이션에 특히 유용합니다.\\n\\n온디바이스 AI\\n\\n프라이버시 보호와 분산 처리 요구가 증가함에 따라 더 많은 RAG 구현이 사용자 기기 내에서 로컬로 작동할 것입니다. 이를 통해 사용자는 자체 데이터 저장소에서 데이터를 처리 및 검색할 수 있어 클라우드 기반 검색 의존도를 줄일 수 있습니다.\\n\\n온디바이스 AI는 외부 데이터 접근 없이 실시간 정보 검색을 가능하게 하여 데이터 보안 향상과 지연 시간 감소에도 기여합니다.\\n\\n능동적 검색 강화 생성\\n\\n생성형 AI 모델은 의미 검색, 벡터 검색, 그래프 임베딩과 같은 정교한 검색 기법을 사용해 관련 문서와 외부 정보원을 적극적으로 추출합니다. AI 애플리케이션은 검색 프로세스를 지속적으로 개선하여 점점 더 정확하고 맥락이 풍부한 콘텐츠를 제공합니다.\\n\\n서비스형 RAG (RAG as a Service)\\n\\n클라우드 기반 RAG 솔루션을 통해 기업은 확장 가능하고 비용 효율적인 RAG 아키텍처를 배포할 수 있습니다. 기업들이 대규모 인프라 투자 없이 AI 기능을 극대화하고 데이터 접근 속도를 높이며 AI 기반 검색 시스템을 워크플로우에 통합가능합니다.\\n\\n향상된 RAG 파이프라인\\n\\n미래의 발전은 AI 모델이 검색 메커니즘에서 외부 데이터 소스에서 관련 데이터를 끌어오는 방식을 개선할 것입니다. 더 나은 검색 강화 기법은 검색 과정을 더욱 간소화하고 AI가 생성하는 답변이 최신이며 정확한 데이터에 기반하도록 보장합니다.\\n\\n마무리\\n\\nRAG는 기존의 언어 모델 한계를 극복하며, 보다 지능적이고 신뢰성 있는 응답을 가능하게 하는 혁신적 기술입니다. 본 글에서는 RAG의 기본 개념부터 기술 구조, 기대 효과, 그리고 구현 시 고려할 점까지 종합적으로 살펴보았습니다.\\n\\nAI 기술의 성공적인 도입은 단순한 도전이 아닌, 기업의 디지털 전환과 경쟁력 강화를 위한 핵심 전략입니다. HBLAB은 베트남의 검증된 IT 파트너로서, RAG 개발 및 AI 아웃소싱 분야에서 풍부한 경험과 전문 인력을 보유하고 있습니다.\\n\\n지금이 바로 HBLAB과 함께 RAG 기반 AI 솔루션을 실현하고, 귀사의 미래를 한 단계 도약시킬 최적의 타이밍입니다.\\n\\n관련 게시물\\n\\nVIETNAM\\n\\n베트남 본사 주소\\n\\n2층 및 21층, C타워, 센트럴 포인트 빌딩, No. 219 Trung Kinh, Cau Giay District, Hanoi\\n\\n다낭 개발 센터\\n\\nTiktak Co-Working Space Da Nang\\n3층, Diamond Time 빌딩, No.35 Thai Phien, Phuoc Ninh, Hai Chau District, 다낭\\n\\n다른\\n\\nSOUTH KOREA\\n\\n한국 지사 주소\\n\\n7층, 서울특별시 강남구 봉은사로 502\\n\\nJAPAN\\n\\n일본 도쿄 지사\\n\\n105-0012 2층, HF 하마마츠쵸 빌딩, 2-12-9 시바 다이몬, 미나토구, 도쿄\\n\\n© 2025HBLAB. 모든 권리 보유.\\n\\n</raw></document>']\n"
     ]
    }
   ],
   "source": [
    "tavily_tool = TavilySearch()\n",
    "\n",
    "search_query = \"RAG를 만든 사람은?\"\n",
    "\n",
    "search_result = tavily_tool.search(\n",
    "    query=search_query,\n",
    "    max_result = 2,\n",
    "    format_output = True\n",
    ")\n",
    "\n",
    "print(search_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36cf8b8e-8386-46c4-b379-2917dc379129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_search(state: GraphState) -> GraphState:\n",
    "    tavily_tool = TavilySearch()\n",
    "    search_query = state[\"question\"]\n",
    "\n",
    "    search_result = tavily_tool.search(\n",
    "        query = search_query,\n",
    "        topic = \"general\",\n",
    "        max_results = 2,\n",
    "        format_output = True,\n",
    "    )\n",
    "\n",
    "    return {\"context\": search_result}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0384117c-563e-4d14-bf36-aad850ee9d4a",
   "metadata": {},
   "source": [
    "## Graph Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "454c4135-db70-4f69-a1e1-cfb94d74fb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"retrieve\", retrieve_document)\n",
    "workflow.add_node(\"relevance_check\", relevance_check)\n",
    "workflow.add_node(\"llm_answer\", llm_answer)\n",
    "workflow.add_node(\"web_search\", web_search)\n",
    "\n",
    "workflow.add_edge(\"retrieve\", \"relevance_check\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"relevance_check\",\n",
    "    is_relevant,\n",
    "    {\n",
    "        \"relevant\":\"llm_answer\",\n",
    "        \"not relevant\":\"web_search\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"web_search\", \"llm_answer\")\n",
    "workflow.add_edge(\"llm_answer\", END)\n",
    "\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "079068c1-4040-4a4b-8e54-b09ebbe23b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.visualize import visualize_graph\n",
    "visualize_graph(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b71db30-ba3d-491e-96e0-a98033bf32b9",
   "metadata": {},
   "source": [
    "## Graph Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8067aab9-65d8-4967-a5c5-711359ae0f12",
   "metadata": {},
   "source": [
    "### Invoking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09866818-1dc6-463c-94ae-06608c0f18b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== [RELEVENCE CHECKING ===\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mrelevance_check\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32mrelevance\u001b[0m:\n",
      "no\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mllm_answer\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\u001b[1;32manswer\u001b[0m:\n",
      "RAG(검색 증강 생성)의 개발자는 특정 개인이나 단체로 명시되어 있지 않습니다. RAG는 기존의 대규모 언어 모델(LLM)의 한계를 극복하기 위해 발전된 기술로, 여러 연구자와 기관의 협력으로 발전해왔습니다.\n",
      "\n",
      "**Source**\n",
      "- [Rag란 무엇인가? 10분만에 이해하기 - 브런치](https://brunch.co.kr/@acc9b16b9f0f430/73)\n",
      "- [Rag (검색 증강 생성)란? - Llm 단점을 보완하는 기술 | 블로그 | 모두의연구소](https://modulabs.co.kr/blog/retrieval-augmented-generation)\n",
      "('user', 'RAG를 만든 사람이 누군지 알려주세요.')\n",
      "('assistant', 'RAG(검색 증강 생성)의 개발자는 특정 개인이나 단체로 명시되어 있지 않습니다. RAG는 기존의 대규모 언어 모델(LLM)의 한계를 극복하기 위해 발전된 기술로, 여러 연구자와 기관의 협력으로 발전해왔습니다.\\n\\n**Source**\\n- [Rag란 무엇인가? 10분만에 이해하기 - 브런치](https://brunch.co.kr/@acc9b16b9f0f430/73)\\n- [Rag (검색 증강 생성)란? - Llm 단점을 보완하는 기술 | 블로그 | 모두의연구소](https://modulabs.co.kr/blog/retrieval-augmented-generation)')\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "\n",
    "config = RunnableConfig(recursion_limit=10, configurable={\"thread_id\": \"1\"})\n",
    "\n",
    "user_input = GraphState(question=\"RAG를 만든 사람이 누군지 알려주세요.\")\n",
    "\n",
    "try:\n",
    "    invoke_graph(app, user_input, config, [\"relevance_check\", \"llm_answer\"])\n",
    "except GraphRecursionError as recursion_error:\n",
    "    print(f\"GraphRecursionError: {recursion_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a829cc03-ca08-4f1d-9f3b-3d1d0c5bd848",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cde6655-3b88-496c-9128-01c3105e3f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mrelevance_check\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{\"score\":\"no\"}=== [RELEVENCE CHECKING ===\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mllm_answer\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "RAG(검색 증강 생성)의 개발자는 특정 개인이나 단체로 명시되어 있지 않으며, 여러 연구자와 기관의 협력으로 발전해온 기술입니다.\n",
      "\n",
      "**Source**\n",
      "- [Rag란 무엇인가? 10분만에 이해하기 - 브런치](https://brunch.co.kr/@acc9b16b9f0f430/73)\n",
      "- [Rag (검색 증강 생성)란? - Llm 단점을 보완하는 기술 | 블로그 | 모두의연구소](https://modulabs.co.kr/blog/retrieval-augmented-generation)"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    stream_graph(app, user_input, config, [\"relevance_check\", \"llm_answer\"])\n",
    "except GraphRecursionError as recursion_error:\n",
    "    print(f\"GraphRecursionError: {recursion_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40af87ec-b0cf-4cc6-b040-57a180bcfed1",
   "metadata": {},
   "source": [
    "### Output Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ba8a3bc-f67b-4ee5-a50f-14e2d8825307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: RAG를 만든 사람이 누군지 알려주세요.\n",
      "==========================================================================================\n",
      "Answer: RAG(검색 증강 생성)의 개발자는 특정 개인이나 단체로 명시되어 있지 않으며, 여러 연구자와 기관의 협력으로 발전해온 기술입니다.\n",
      "\n",
      "**Source**\n",
      "- [Rag란 무엇인가? 10분만에 이해하기 - 브런치](https://brunch.co.kr/@acc9b16b9f0f430/73)\n",
      "- [Rag (검색 증강 생성)란? - Llm 단점을 보완하는 기술 | 블로그 | 모두의연구소](https://modulabs.co.kr/blog/retrieval-augmented-generation)\n"
     ]
    }
   ],
   "source": [
    "output = app.get_state(config).values\n",
    "\n",
    "print(f'Question: {output[\"question\"]}')\n",
    "print(\"===\" * 30)\n",
    "print(f'Answer: {output[\"answer\"]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fe40d2-6f91-40de-a4b6-d46558d887e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064f25e2-6bbf-4f0c-a00d-0e38f9ec40f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
